{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac40623",
   "metadata": {},
   "source": [
    "# Workshop: De cero a Agente con LangChain y Python\n",
    "\n",
    "En este workshop de 4 horas aprenderás a construir un agente inteligente desde cero utilizando **LangChain**, **Python** y modelos de lenguaje **open source** como **Ollama** o **LM Studio**. A lo largo del taller veremos los conceptos básicos de los LLMs, cómo orquestar herramientas y memorias con LangChain, y cómo integrar un vector store para recuperación aumentada de datos (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8227ed",
   "metadata": {},
   "source": [
    "## Agenda del Workshop\n",
    "\n",
    "1. Introducción a los LLMs y la ejecución local\n",
    "2. Componentes de LangChain (modelos, chains, herramientas)\n",
    "3. Definición de herramientas personalizadas\n",
    "4. Configuración de un modelo local (Ollama / LM Studio)\n",
    "5. Creación de un agente reactivo con memoria\n",
    "6. Integración con un vector store para RAG\n",
    "7. Demostración final y conclusiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4245e",
   "metadata": {},
   "source": [
    "## Conexión directa a un LLM sin LangChain\n",
    "\n",
    "Antes de introducir LangChain, veamos cómo podríamos interactuar con un modelo de lenguaje utilizando únicamente el SDK o la API que proporciona el modelo. Por ejemplo, un servidor de Ollama expone un endpoint HTTP `http://localhost:11434/api/generate` donde puedes enviar un prompt y recibir la respuesta del modelo en formato JSON. De forma análoga, otros proveedores (como OpenAI) ofrecen SDKs o endpoints REST para invocar sus modelos.\n",
    "\n",
    "Interactuar de forma directa es útil para pruebas sencillas, pero pronto verás que gestionar memoria de conversación, combinar varios modelos, reintentar peticiones o integrar fuentes de datos externas se vuelve complejo. Aquí es donde entra LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46316392",
   "metadata": {},
   "source": [
    "# Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55c6699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "**LangChain** es una herramienta de inteligencia artificial que permite al usuario crear y explorar el mundo de la lógica deductiva, el razonamiento de deducción. Utiliza el algoritmo de machine learning para inferir las conclusiones necesarias para completar un enunciado o teoría que se basa en una serie de oraciones lógicamente coherentes.\n",
      "\n",
      "### Características principales del langchain:\n",
      "1. **Conjunción Clave**: El usuario debe provide una serie de oraciones (conjunto de axiomas) que reflejen un conjunto de ideas fijas y bien definidas.\n",
      "2. **Deducción Lógica**: El langchain infiere las conclusiones necesarias para completar el enunciado o teoría proporcionada, ofreciendo respuestas claras e instructivas.\n",
      "3. **Efectividad en la Enseñanza**: Es una herramienta útil en la enseñanza de lógica, matemáticas y razonamiento deductivo.\n",
      "\n",
      "### Ejemplo:\n",
      "\"El solveo está a la derecha del refugio. El refugio está en el corazón de las ciudades.\"  \n",
      "**langchain**: \"Si el solveo está a la derecha del refugio y el refugio está en el corazón de las ciudades, entonces...\" (conclusione).\n",
      "\n",
      "La langchain no puede inferir lógicamente la información dada, solo proporcionará respuestas claras e instructivas.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de llamada directa a un modelo local de Ollama\n",
    "import requests\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"prompt\": \"Que es langchain?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petición POST al endpoint de Ollama\n",
    "# (Nota: esta llamada sólo funcionará si tienes ollama corriendo de forma local)\n",
    "response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "print(response.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0303e",
   "metadata": {},
   "source": [
    "# OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d789a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (1.75.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b5cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain** es un *framework de código abierto* diseñado para simplificar el desarrollo de aplicaciones utilizando **modelos de lenguaje grandes (LLMs)** como GPT-3, GPT-4, Llama, Claude y otros. Su objetivo principal es conectar estos modelos con fuentes externas (bases de datos, APIs, documentos) y permitir la creación de flujos complejos de razonamiento (**cadenas o *chains***).\n",
      "\n",
      "---\n",
      "\n",
      "### **Problema que resuelve:**\n",
      "Los LLMs por sí solos tienen limitaciones:  \n",
      "- No acceden a datos en tiempo real.  \n",
      "- No interactúan con herramientas externas (búsquedas, cálculos).  \n",
      "- Carecen de memoria entre interacciones.  \n",
      "- Manipular prompts complejos es complicado.  \n",
      "\n",
      "**LangChain integra los LLMs en sistemas más robustos.**\n",
      "\n",
      "---\n",
      "\n",
      "### **Componentes clave:**\n",
      "1. **Models:**  \n",
      "   Soporte para múltiples LLMs y embeddings (como OpenAI, Hugging Face).\n",
      "\n",
      "2. **Prompts:**  \n",
      "   Gestión avanzada de prompts (plantillas, optimización, few-shot learning).\n",
      "\n",
      "3. **Chains:**  \n",
      "   Secuencias predefinidas o personalizadas de llamadas a LLMs y otras herramientas.  \n",
      "   Ejemplo:  \n",
      "   `LLM → Buscar en Wikipedia → Sintetizar respuesta`.\n",
      "\n",
      "4. **Agents:**  \n",
      "   Entidades que usan LLMs para decidir *acciones* (por ejemplo: \"Busca en Google\", \"Ejecuta código\").  \n",
      "   Herramientas comunes: Python REPL, APIs, motores de búsqueda.\n",
      "\n",
      "5. **Memory:**  \n",
      "   Mantiene el estado entre interacciones (ej.: recordar el contexto en un chat).\n",
      "\n",
      "6. **Indexes:**  \n",
      "   Conecta LLMs con tus datos (documentos, bases de datos mediante *RAG*).\n",
      "\n",
      "---\n",
      "\n",
      "### **Casos de uso:**\n",
      "- **Chatbots con contexto** (respuestas basadas en documentos propios).  \n",
      "- **Asistentes automáticos** que ejecutan tareas (analizar datos, resumir emails).  \n",
      "- **Análisis de documentos** (PDFs, hojas de cálculo).  \n",
      "- **Desarrollo de agentes autónomos** que toman decisiones iterativas.\n",
      "\n",
      "---\n",
      "\n",
      "### **Ejemplo mínimo en código (Python):**\n",
      "```python\n",
      "from langchain_openai import OpenAI\n",
      "from langchain.chains import LLMChain\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "plantilla = \"Traduce al {idioma}: {texto}\"\n",
      "prompt = PromptTemplate.from_template(plantilla)\n",
      "\n",
      "llm = OpenAI(api_key=\"tu_api_key\")\n",
      "cadena = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "resultado = cadena.invoke({\"idioma\": \"francés\", \"texto\": \"Hola, ¿cómo estás?\"})\n",
      "print(resultado[\"text\"])  # Output: \"Bonjour, comment ça va ?\"\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Ventajas:**\n",
      "- **Abierto y flexible:** Compatible con +50 integraciones (OpenAI, Google, Azure, etc.).  \n",
      "- **Comunidad activa:** 60k+ estrellas en GitHub (2024) y amplia documentación.  \n",
      "- **Ideal para RAG (*Retrieval-Augmented Generation*):** Respuestas precisas usando datos externos.\n",
      "\n",
      "**Plataforma alternativa:** Haystack, LlamaIndex, Semantic Kernel.\n",
      "\n",
      "LangChain es esencial para llevar los LLMs más allá de un chat básico, creando aplicaciones de IA contextuales y potentes. 🚀\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"MI PAGINA o APP\", \n",
    "    \"X-Title\": \"ANDY CODE\",\n",
    "  },\n",
    "  model=\"deepseek/deepseek-r1-0528:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Que es langchain?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14f5ab",
   "metadata": {},
   "source": [
    "# Google SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0134f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e164470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It learns patterns from data to make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b51ae",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Por qué utilizar LangChain?\n",
    "\n",
    "Aunque podrías interactuar directamente con un modelo de lenguaje usando el SDK que ofrece cada proveedor (por ejemplo la API de **OpenAI**, el servidor local de **Ollama** o el endpoint de **LM Studio**), **LangChain** proporciona una capa de abstracción y orquestación muy útil cuando necesitas construir agentes más sofisticados:\n",
    "\n",
    "- **Unifica interfaces**: te permite cambiar entre distintos LLMs (open source o propietarios) sin modificar el resto de tu código, porque expone una API común para modelos de chat, embeddings y vector stores.\n",
    "- **Encadena tareas**: facilita construir *chains* donde la salida de una llamada se usa como entrada de otra, incluyendo flujos de preguntas y respuestas, análisis de datos o ejecución de herramientas externas.\n",
    "- **Gestión de memoria**: ofrece componentes para almacenar el historial de conversaciones y recuperarlo, algo esencial para agentes conversacionales.\n",
    "- **Integración de herramientas**: permite exponer funciones personalizadas (cálculos, búsquedas, consultas API, etc.) como herramientas que el modelo puede invocar cuando es necesario.\n",
    "- **Recuperación aumentada (RAG)**: se integra con motores de vector y sistemas de embeddings para buscar documentos relevantes y combinarlos con la generación del LLM.\n",
    "\n",
    "En resumen, LangChain actúa como el pegamento que conecta los diferentes bloques (modelos, herramientas, bases de datos) y te permite centrarte en la lógica de tu agente en lugar de los detalles de cada SDK.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3af0",
   "metadata": {},
   "source": [
    "### Instalación y configuración\n",
    "\n",
    "Para seguir este notebook necesitas instalar varias librerías. Si ya tienes un entorno con `langchain` y `chromadb` puedes omitir esta celda. En una máquina local con acceso a internet se pueden instalar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c8229",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-community langchain-core chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a185c",
   "metadata": {},
   "source": [
    "### Importación de módulos\n",
    "\n",
    "Importamos las clases y funciones necesarias para construir el agente. Esto incluye el modelo local (por ejemplo `ChatOllama`), el motor de memoria, las herramientas y funciones auxiliares de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool, tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861e6ec",
   "metadata": {},
   "source": [
    "## Cadenas en LangChain\n",
    "\n",
    "Una **cadena** combina uno o más componentes (prompts, modelos, transformaciones) para construir un flujo de ejecución. LangChain incluye utilidades como `LLMChain` para encapsular un prompt y un modelo. Aquí tienes un ejemplo de cadena simple que genera una respuesta a partir de un template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cadena simple\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definimos un prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Dime un dato curioso sobre {tema}.\")\n",
    "\n",
    "# Instanciamos el modelo local (suponiendo que esté en marcha)\n",
    "llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Creamos la cadena\n",
    "cadena = LLMChain(llm=llm_local, prompt=prompt)\n",
    "\n",
    "# Para ejecutarla proporcionaríamos las variables del template:\n",
    "# resultado = cadena.invoke({\"tema\": \"Colombia\"})\n",
    "# print(resultado)\n",
    "# Nota: descomenta estas líneas para ejecutar con un modelo local en marcha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b09f95",
   "metadata": {},
   "source": [
    "## Memoria en LangChain\n",
    "\n",
    "Los agentes conversacionales necesitan recordar lo que ya se ha dicho. LangChain ofrece varias implementaciones de memoria, como `ConversationBufferMemory`, que almacena el historial de mensajes en orden. Puedes combinarla con un LLMChain o un agente para que el modelo reciba contexto en cada llamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de ConversationBufferMemory con un LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Creamos la memoria\n",
    "memoria_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Definimos el prompt\n",
    "prompt_memoria = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente amistoso.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{chat_history}\"),\n",
    "])\n",
    "\n",
    "# Creamos la cadena con memoria\n",
    "cadena_memoria = LLMChain(llm=llm_local, prompt=prompt_memoria, memory=memoria_chain)\n",
    "\n",
    "# Para usarla, invoca la cadena varias veces; la memoria conservará el historial:\n",
    "# respuesta1 = cadena_memoria.invoke({\"input\": \"Hola\"})\n",
    "# respuesta2 = cadena_memoria.invoke({\"input\": \"¿Qué me dijiste antes?\"})\n",
    "# print(respuesta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8351375",
   "metadata": {},
   "source": [
    "## Plantillas de Prompt (Prompt Templates)\n",
    "\n",
    "LangChain facilita la construcción de prompts complejos mediante plantillas parametrizadas. Puedes combinar mensajes de sistema, de usuario y del asistente para definir el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba42c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de plantilla de prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "plantilla = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en matemáticas.\"),\n",
    "    (\"human\", \"Pregunta: {pregunta}\"),\n",
    "])\n",
    "\n",
    "# Instanciaríamos el modelo y llamaríamos:\n",
    "# llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "# chain = LLMChain(llm=llm_local, prompt=plantilla)\n",
    "# respuesta = chain.invoke({\"pregunta\": \"¿Cuánto es 12×8?\"})\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906b3f",
   "metadata": {},
   "source": [
    "## 1. Definir herramientas personalizadas\n",
    "\n",
    "Las **herramientas** permiten que el agente ejecute funciones específicas (por ejemplo cálculos o búsquedas). Definiremos dos funciones sencillas que suman y multiplican números. Utilizamos el decorador `@tool` para convertirlas en herramientas que LangChain pueda invocar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    # Suma dos números y devuelve el resultado\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    # Multiplica dos números y devuelve el resultado\n",
    "    return a * b\n",
    "\n",
    "# Registrar las herramientas en una lista\n",
    "herramientas = [sumar, multiplicar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696df71",
   "metadata": {},
   "source": [
    "## 2. Instanciar el modelo local\n",
    "\n",
    "Antes de crear el agente necesitamos un LLM local en ejecución. Con **Ollama** podemos iniciar un servidor y cargar un modelo como *llama3:8b*:\n",
    "\n",
    "```bash\n",
    "# Instala Ollama (solo una vez)\n",
    "wget -qO- https://ollama.com/install.sh | sh\n",
    "# Arranca el servidor\n",
    "ollama serve &\n",
    "# Descarga y prepara el modelo (puede tardar unos minutos)\n",
    "ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "En **LM Studio** puedes descargar modelos desde la interfaz gráfica y exponer un endpoint local. Una vez en marcha, LangChain se conecta mediante la clase `ChatOllama` indicando el nombre del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2698f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el modelo local (asegúrate de que el servidor de Ollama esté ejecutándose)\n",
    "# Si utilizas LM Studio, cambia el nombre del modelo o ajusta el endpoint.\n",
    "llm = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# También puedes ajustar parámetros como temperatura, top_p, etc.\n",
    "# llm = ChatOllama(model=\"llama3:8b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38857",
   "metadata": {},
   "source": [
    "## 3. Crear memoria y construir el agente\n",
    "\n",
    "La memoria mantiene el historial de conversación. Usaremos `ConversationBufferMemory` para recordar los mensajes pasados. Luego construiremos un **agente reactivo** con `create_react_agent`, pasando el modelo, la lista de herramientas y la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una memoria de conversación\n",
    "memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Crear el prompt que usará el agente\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil que puede usar herramientas para completar tareas.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente reactivo (ReAct)\n",
    "agente = create_react_agent(llm, herramientas, prompt)\n",
    "\n",
    "# Ejecutar el agente dentro de un executor para gestionar el estado y la memoria\n",
    "executor = AgentExecutor(agent=agente, tools=herramientas, memory=memoria, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb9f",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el agente\n",
    "\n",
    "Ya podemos hacer consultas al agente. El agente decidirá si necesita llamar a alguna herramienta para calcular o buscar información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta: combinación de cálculo\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuánto es 5*7 y 3*8?\"}]}\n",
    "# Para probar el agente descomenta las líneas siguientes cuando tengas el modelo local ejecutándose:\n",
    "respuesta = executor.invoke(pregunta)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62883c8f",
   "metadata": {},
   "source": [
    "## 5. Ejemplo de agente calculadora\n",
    "\n",
    "Construiremos un agente que sólo utiliza herramientas aritméticas para resolver preguntas de cálculo. El agente decidirá si debe llamar a la función de suma o multiplicación en función de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuevamente nuestras herramientas aritméticas\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    return a * b\n",
    "\n",
    "# Lista de herramientas\n",
    "calculadora_tools = [sumar, multiplicar]\n",
    "\n",
    "# Instanciamos un modelo local (por ejemplo, llama3)\n",
    "llm_calc = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Memoria para el agente\n",
    "mem_calc = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt base\n",
    "prompt_calc = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres una calculadora que puede usar herramientas para sumar y multiplicar.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear agente y executor\n",
    "agente_calc = create_react_agent(llm_calc, calculadora_tools, prompt_calc)\n",
    "executor_calc = AgentExecutor(agent=agente_calc, tools=calculadora_tools, memory=mem_calc, verbose=True)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuánto es 4*6 más 10?\"}]}\n",
    "# respuesta = executor_calc.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693985",
   "metadata": {},
   "source": [
    "## 6. Agente de búsqueda de artículos científicos con Playwright, Gemini y Ollama\n",
    "\n",
    "En este ejemplo final combinamos la automatización web mediante **Playwright** con dos modelos de lenguaje distintos. El agente navega a una base de datos de artículos (por ejemplo Google Scholar), realiza una búsqueda de artículos científicos sobre un tema y extrae los títulos. Luego utiliza un modelo de **Gemini** para filtrar y evaluar la relevancia de los resultados, y finalmente emplea un modelo local de **Ollama** para sintetizar una respuesta para el usuario.\n",
    "\n",
    "**Nota:** Playwright necesita instalarse (`pip install playwright` y luego `playwright install`), y el acceso a Gemini requiere configurar credenciales de Google generative AI. Este ejemplo es ilustrativo y no se ejecutará en este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de agente que usa Playwright para buscar artículos y combina dos modelos\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "@tool\n",
    "def buscar_articulos(topic: str) -> str:\n",
    "    \"\"\"Busca artículos científicos sobre un tema utilizando Playwright y devuelve los títulos encontrados.\"\"\"\n",
    "    from playwright.sync_api import sync_playwright\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        # Navegar a Google Scholar\n",
    "        page.goto(\"https://scholar.google.com\")\n",
    "        page.fill(\"input[name=q]\", topic)\n",
    "        page.press(\"input[name=q]\", \"Enter\")\n",
    "        page.wait_for_selector(\"h3\")\n",
    "        # Extraer los primeros 5 títulos\n",
    "        titles = page.eval_on_selector_all(\"h3\", \"elements => elements.slice(0,5).map(e => e.innerText)\")\n",
    "        browser.close()\n",
    "        return \"\".join(titles)\n",
    "\n",
    "# Instanciamos los modelos\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "llm_ollama = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Lista de herramientas\n",
    "tools_playwright = [buscar_articulos]\n",
    "\n",
    "# Prompt base para el agente\n",
    "prompt_playwright = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente investigador que puede usar herramientas para buscar y evaluar artículos.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente que decide con Gemini\n",
    "agente_playwright = create_react_agent(llm_gemini, tools_playwright, prompt_playwright)\n",
    "\n",
    "# Executor que sintetiza la respuesta final usando Ollama\n",
    "mem_playwright = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "executor_playwright = AgentExecutor(agent=agente_playwright, tools=tools_playwright, memory=mem_playwright, verbose=True, llm=llm_ollama)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"Busca artículos sobre inteligencia artificial explicable.\"}]}\n",
    "# respuesta = executor_playwright.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
