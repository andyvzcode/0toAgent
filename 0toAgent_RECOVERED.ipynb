{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac40623",
   "metadata": {},
   "source": [
    "# Workshop: De cero a Agente con LangChain y Python\n",
    "\n",
    "En este workshop de 4 horas aprender√°s a construir un agente inteligente desde cero utilizando **LangChain**, **Python** y modelos de lenguaje **open source** como **Ollama** o **LM Studio**. A lo largo del taller veremos los conceptos b√°sicos de los LLMs, c√≥mo orquestar herramientas y memorias con LangChain, y c√≥mo integrar un vector store para recuperaci√≥n aumentada de datos (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8227ed",
   "metadata": {},
   "source": [
    "## Agenda del Workshop\n",
    "\n",
    "1. Introducci√≥n a los LLMs y la ejecuci√≥n local\n",
    "2. Componentes de LangChain (modelos, chains, herramientas)\n",
    "3. Definici√≥n de herramientas personalizadas\n",
    "4. Configuraci√≥n de un modelo local (Ollama / LM Studio)\n",
    "5. Creaci√≥n de un agente reactivo con memoria\n",
    "6. Integraci√≥n con un vector store para RAG\n",
    "7. Demostraci√≥n final y conclusiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4245e",
   "metadata": {},
   "source": [
    "## Conexi√≥n directa a un LLM sin LangChain\n",
    "\n",
    "Antes de introducir LangChain, veamos c√≥mo podr√≠amos interactuar con un modelo de lenguaje utilizando √∫nicamente el SDK o la API que proporciona el modelo. Por ejemplo, un servidor de Ollama expone un endpoint HTTP `http://localhost:11434/api/generate` donde puedes enviar un prompt y recibir la respuesta del modelo en formato JSON. De forma an√°loga, otros proveedores (como OpenAI) ofrecen SDKs o endpoints REST para invocar sus modelos.\n",
    "\n",
    "Interactuar de forma directa es √∫til para pruebas sencillas, pero pronto ver√°s que gestionar memoria de conversaci√≥n, combinar varios modelos, reintentar peticiones o integrar fuentes de datos externas se vuelve complejo. Aqu√≠ es donde entra LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46316392",
   "metadata": {},
   "source": [
    "# Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c6699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Un **LangChain** es un sistema que combina el poder del lenguaje y el poder del computado para permitir la generaci√≥n autom√°tica de c√≥digo. Diferente entre diferentes tama√±os, se puede pensar en:\n",
      "\n",
      "1. **Lenguaje:** Es lo que es native al lenguaje de una language chain (llamado lenguaje base), como Java, Python, C# o JavaScript.\n",
      "2. **Computo:** Es lo que es native del compute, como un red hBACKslash, el servidor, la server y el red h front. \n",
      "\n",
      "En resumen, un langchain da el poder al lenguaje para que se use como una interfaz para el computador, permitiendo que el computador genere c√≥digo autom√°ticamente en la forma de la lenguaje base que es m√°s sutil e intuitiva.\n",
      "\n",
      "¬øQu√© te parece ser el mejor langchain para tu needs?\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de llamada directa a un modelo local de Ollama\n",
    "import requests\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"prompt\": \"Que es langchain?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petici√≥n POST al endpoint de Ollama\n",
    "# (Nota: esta llamada s√≥lo funcionar√° si tienes ollama corriendo de forma local)\n",
    "response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "print(response.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0303e",
   "metadata": {},
   "source": [
    "# OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d789a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (1.75.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain** es un *framework de c√≥digo abierto* dise√±ado para facilitar el desarrollo de aplicaciones basadas en **modelos de lenguaje** (como GPT de OpenAI, Llama 2, etc.). Su objetivo principal es conectar estos modelos con fuentes externas de datos y permitir su integraci√≥n en \"cadenas\" de acciones complejas para aplicaciones m√°s robustas.  \n",
      "\n",
      "### Caracter√≠sticas clave:\n",
      "1. **Modelos (Models)**:  \n",
      "   - Soporta m√∫ltiples modelos de lenguaje (GPT, Hugging Face, etc.) y embeddings.\n",
      "\n",
      "2. **Componentes modulares**:  \n",
      "   - **Prompts**: Administra plantillas de texto para interactuar con los modelos.  \n",
      "   - **Memoria**: Guarda el contexto de conversaciones (√∫til para chatbots).  \n",
      "   - **Agentes**: Modelos que toman decisiones para ejecutar herramientas (b√∫squedas web, c√°lculos, APIs).  \n",
      "   - **Retrieval**: Conecta con bases de datos o documentos (PDFs, web) para respuestas basadas en datos reales.  \n",
      "\n",
      "3. **Cadenas (Chains)**:  \n",
      "   - Combina m√∫ltiples pasos (ej: \"consultar un modelo ‚Üí procesar resultado ‚Üí llamar a una API\") en un flujo secuencial.  \n",
      "\n",
      "### ¬øPara qu√© sirve?\n",
      "- **Chatbots avanzados** con memoria contextual.  \n",
      "- **Asistentes personalizados** que usan tus documentos (ej: responder preguntas sobre un archivo PDF).  \n",
      "- **Automatizaci√≥n** de tareas complejas que requieren razonamiento.  \n",
      "- Desarrollo r√°pido de **prototipos** con LLMs.  \n",
      "\n",
      "### Ejemplo m√≠nimo:\n",
      "```python\n",
      "from langchain_community.llms import OpenAI\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "llm = OpenAI(api_key=\"...\")  \n",
      "prompt = \"Traduce al espa√±ol: 'Hello world'\"\n",
      "respuesta = llm.invoke(prompt)\n",
      "print(respuesta)  # \"Hola mundo\"\n",
      "```\n",
      "\n",
      "üí° **Ventaja clave**:  \n",
      "Evita reinventar funcionalidades comunes (gesti√≥n de contexto, integraciones), acelerando el desarrollo.  \n",
      "\n",
      "**Lenguajes**: Principalmente Python y TypeScript.  \n",
      "**GitHub**: [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)  \n",
      "\n",
      "¬øQuieres un ejemplo pr√°ctico paso a paso o c√≥mo usar alguna funci√≥n espec√≠fica?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"MI PAGINA o APP\", \n",
    "    \"X-Title\": \"ANDY CODE\",\n",
    "  },\n",
    "  model=\"deepseek/deepseek-r1-0528:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Que es langchain?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14f5ab",
   "metadata": {},
   "source": [
    "# Google SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0134f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e164470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain es un **framework de c√≥digo abierto** que simplifica la creaci√≥n de aplicaciones impulsadas por **Grandes Modelos de Lenguaje (LLMs)**. En esencia, act√∫a como un puente entre los poderosos LLMs (como GPT-4, Llama 2, Claude, etc.) y tus propias fuentes de datos o herramientas externas, permiti√©ndote construir aplicaciones mucho m√°s sofisticadas y personalizadas.\n",
      "\n",
      "Imagina que un LLM es un cerebro incre√≠blemente inteligente, pero que por s√≠ solo tiene limitaciones:\n",
      "1.  **Conocimiento Limitado:** Su conocimiento se detiene en la fecha de su √∫ltimo entrenamiento y no tiene acceso a informaci√≥n en tiempo real o privada.\n",
      "2.  **Incapacidad para Actuar:** No puede buscar en internet, interactuar con APIs, bases de datos o ejecutar c√≥digo.\n",
      "3.  **Procesos de M√∫ltiples Pasos:** Aunque puede razonar, a menudo necesita descomponer tareas complejas en varios pasos y tomar decisiones en cada uno.\n",
      "\n",
      "**LangChain aborda estas limitaciones** al proporcionar un conjunto de herramientas y componentes que permiten a los desarrolladores:\n",
      "\n",
      "### ¬øQu√© te permite hacer LangChain?\n",
      "\n",
      "1.  **Conectar LLMs con Fuentes de Datos Externas (RAG - Retrieval Augmented Generation):**\n",
      "    *   Puedes cargar documentos, p√°ginas web, bases de datos, etc.\n",
      "    *   LangChain te ayuda a indexar esta informaci√≥n (a menudo usando bases de datos vectoriales).\n",
      "    *   Cuando un usuario hace una pregunta, LangChain puede buscar la informaci√≥n relevante en tus datos y luego pasarla al LLM para que genere una respuesta informada, superando el conocimiento pre-entrenado del modelo.\n",
      "\n",
      "2.  **Utilizar Herramientas y APIs Externas:**\n",
      "    *   Permite que el LLM interact√∫e con el mundo exterior. Por ejemplo, un LLM podr√≠a usar una herramienta para:\n",
      "        *   Buscar en Google.\n",
      "        *   Consultar el clima.\n",
      "        *   Enviar un email.\n",
      "        *   Interactuar con una base de datos de tu empresa.\n",
      "        *   Ejecutar c√≥digo Python para c√°lculos complejos.\n",
      "\n",
      "3.  **Construir Agentes LLM:**\n",
      "    *   Un agente es un LLM que tiene la capacidad de **decidir qu√© herramientas usar y en qu√© orden** para lograr un objetivo. En lugar de simplemente responder a una pregunta, un agente puede planificar una secuencia de acciones para resolver un problema complejo.\n",
      "\n",
      "4.  **Crear Cadenas (Chains) de Operaciones:**\n",
      "    *   Una cadena es una secuencia de llamadas. Por ejemplo, puedes tener una cadena que primero formatea una entrada (prompt template), luego la pasa a un LLM, y la salida de ese LLM la pasa a otro LLM o a una herramienta. Esto permite construir flujos de trabajo complejos.\n",
      "\n",
      "5.  **Gestionar la Memoria y el Historial de Conversaci√≥n:**\n",
      "    *   Es fundamental para chatbots. LangChain proporciona m√≥dulos para que los LLMs \"recuerden\" conversaciones pasadas, manteniendo el contexto en interacciones multi-turno.\n",
      "\n",
      "6.  **Gestionar y Optimizar Prompts:**\n",
      "    *   Ofrece plantillas para prompts y utilidades para construir prompts din√°micamente, lo que facilita la experimentaci√≥n y la mejora de las interacciones con el LLM.\n",
      "\n",
      "### Componentes Clave de LangChain:\n",
      "\n",
      "*   **Models:** Interfaces para interactuar con diferentes proveedores de LLMs (OpenAI, Hugging Face, Cohere, etc.) y modelos de embedding.\n",
      "*   **Prompts:** Herramientas para construir, gestionar y optimizar los prompts que se env√≠an a los LLMs.\n",
      "*   **Chains:** Permiten encadenar diferentes componentes para crear flujos de trabajo complejos.\n",
      "*   **Retrieval:** Facilita la conexi√≥n y consulta de datos externos.\n",
      "*   **Agents:** Permiten que los LLMs tomen decisiones sobre qu√© acciones realizar utilizando herramientas.\n",
      "*   **Memory:** Para mantener el estado y el historial de las conversaciones.\n",
      "*   **Tools:** Funcionalidades que los agentes pueden usar (por ejemplo, b√∫squeda web, calculadora, etc.).\n",
      "\n",
      "### ¬øPor qu√© es importante LangChain?\n",
      "\n",
      "*   **Simplifica la Complejidad:** Abstrae gran parte de la complejidad de integrar LLMs con otros sistemas.\n",
      "*   **Modularidad:** Permite construir aplicaciones combinando componentes como bloques de LEGO.\n",
      "*   **Flexibilidad:** Funciona con una amplia gama de LLMs, bases de datos y herramientas.\n",
      "*   **Acelera el Desarrollo:** Reduce el tiempo y el esfuerzo necesarios para construir aplicaciones robustas con LLMs.\n",
      "*   **Potencia a los LLMs:** Les permite ir m√°s all√° de su conocimiento pre-entrenado y realizar acciones en el mundo real.\n",
      "\n",
      "En resumen, LangChain democratiza la construcci√≥n de aplicaciones avanzadas con LLMs, permitiendo a los desarrolladores crear sistemas inteligentes que pueden interactuar con informaci√≥n en tiempo real, ejecutar acciones y mantener conversaciones contextualizadas.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client(api_key=\"AIzaSyDKPdCv74mFw9TsjWnqjWLTazlBSIncocs\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents= \"Que es langchain?\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b51ae",
   "metadata": {},
   "source": [
    "\n",
    "## ¬øPor qu√© utilizar LangChain?\n",
    "\n",
    "Aunque podr√≠as interactuar directamente con un modelo de lenguaje usando el SDK que ofrece cada proveedor (por ejemplo la API de **OpenAI**, el servidor local de **Ollama** o el endpoint de **LM Studio**), **LangChain** proporciona una capa de abstracci√≥n y orquestaci√≥n muy √∫til cuando necesitas construir agentes m√°s sofisticados:\n",
    "\n",
    "- **Unifica interfaces**: te permite cambiar entre distintos LLMs (open source o propietarios) sin modificar el resto de tu c√≥digo, porque expone una API com√∫n para modelos de chat, embeddings y vector stores.\n",
    "- **Encadena tareas**: facilita construir *chains* donde la salida de una llamada se usa como entrada de otra, incluyendo flujos de preguntas y respuestas, an√°lisis de datos o ejecuci√≥n de herramientas externas.\n",
    "- **Gesti√≥n de memoria**: ofrece componentes para almacenar el historial de conversaciones y recuperarlo, algo esencial para agentes conversacionales.\n",
    "- **Integraci√≥n de herramientas**: permite exponer funciones personalizadas (c√°lculos, b√∫squedas, consultas API, etc.) como herramientas que el modelo puede invocar cuando es necesario.\n",
    "- **Recuperaci√≥n aumentada (RAG)**: se integra con motores de vector y sistemas de embeddings para buscar documentos relevantes y combinarlos con la generaci√≥n del LLM.\n",
    "\n",
    "En resumen, LangChain act√∫a como el pegamento que conecta los diferentes bloques (modelos, herramientas, bases de datos) y te permite centrarte en la l√≥gica de tu agente en lugar de los detalles de cada SDK.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3af0",
   "metadata": {},
   "source": [
    "### Instalaci√≥n y configuraci√≥n\n",
    "\n",
    "Para seguir este notebook necesitas instalar varias librer√≠as. Si ya tienes un entorno con `langchain` y `chromadb` puedes omitir esta celda. En una m√°quina local con acceso a internet se pueden instalar as√≠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c8229",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-community langchain-core chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a185c",
   "metadata": {},
   "source": [
    "### Importaci√≥n de m√≥dulos\n",
    "\n",
    "Importamos las clases y funciones necesarias para construir el agente. Esto incluye el modelo local (por ejemplo `ChatOllama`), el motor de memoria, las herramientas y funciones auxiliares de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool, tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861e6ec",
   "metadata": {},
   "source": [
    "## Cadenas en LangChain\n",
    "\n",
    "Una **cadena** combina uno o m√°s componentes (prompts, modelos, transformaciones) para construir un flujo de ejecuci√≥n. LangChain incluye utilidades como `LLMChain` para encapsular un prompt y un modelo. Aqu√≠ tienes un ejemplo de cadena simple que genera una respuesta a partir de un template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cadena simple\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definimos un prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Dime un dato curioso sobre {tema}.\")\n",
    "\n",
    "# Instanciamos el modelo local (suponiendo que est√© en marcha)\n",
    "llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Creamos la cadena\n",
    "cadena = LLMChain(llm=llm_local, prompt=prompt)\n",
    "\n",
    "# Para ejecutarla proporcionar√≠amos las variables del template:\n",
    "# resultado = cadena.invoke({\"tema\": \"Colombia\"})\n",
    "# print(resultado)\n",
    "# Nota: descomenta estas l√≠neas para ejecutar con un modelo local en marcha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b09f95",
   "metadata": {},
   "source": [
    "## Memoria en LangChain\n",
    "\n",
    "Los agentes conversacionales necesitan recordar lo que ya se ha dicho. LangChain ofrece varias implementaciones de memoria, como `ConversationBufferMemory`, que almacena el historial de mensajes en orden. Puedes combinarla con un LLMChain o un agente para que el modelo reciba contexto en cada llamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de ConversationBufferMemory con un LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Creamos la memoria\n",
    "memoria_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Definimos el prompt\n",
    "prompt_memoria = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente amistoso.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{chat_history}\"),\n",
    "])\n",
    "\n",
    "# Creamos la cadena con memoria\n",
    "cadena_memoria = LLMChain(llm=llm_local, prompt=prompt_memoria, memory=memoria_chain)\n",
    "\n",
    "# Para usarla, invoca la cadena varias veces; la memoria conservar√° el historial:\n",
    "# respuesta1 = cadena_memoria.invoke({\"input\": \"Hola\"})\n",
    "# respuesta2 = cadena_memoria.invoke({\"input\": \"¬øQu√© me dijiste antes?\"})\n",
    "# print(respuesta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8351375",
   "metadata": {},
   "source": [
    "## Plantillas de Prompt (Prompt Templates)\n",
    "\n",
    "LangChain facilita la construcci√≥n de prompts complejos mediante plantillas parametrizadas. Puedes combinar mensajes de sistema, de usuario y del asistente para definir el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba42c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de plantilla de prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "plantilla = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en matem√°ticas.\"),\n",
    "    (\"human\", \"Pregunta: {pregunta}\"),\n",
    "])\n",
    "\n",
    "# Instanciar√≠amos el modelo y llamar√≠amos:\n",
    "# llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "# chain = LLMChain(llm=llm_local, prompt=plantilla)\n",
    "# respuesta = chain.invoke({\"pregunta\": \"¬øCu√°nto es 12√ó8?\"})\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906b3f",
   "metadata": {},
   "source": [
    "## 1. Definir herramientas personalizadas\n",
    "\n",
    "Las **herramientas** permiten que el agente ejecute funciones espec√≠ficas (por ejemplo c√°lculos o b√∫squedas). Definiremos dos funciones sencillas que suman y multiplican n√∫meros. Utilizamos el decorador `@tool` para convertirlas en herramientas que LangChain pueda invocar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    # Suma dos n√∫meros y devuelve el resultado\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    # Multiplica dos n√∫meros y devuelve el resultado\n",
    "    return a * b\n",
    "\n",
    "# Registrar las herramientas en una lista\n",
    "herramientas = [sumar, multiplicar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696df71",
   "metadata": {},
   "source": [
    "## 2. Instanciar el modelo local\n",
    "\n",
    "Antes de crear el agente necesitamos un LLM local en ejecuci√≥n. Con **Ollama** podemos iniciar un servidor y cargar un modelo como *llama3:8b*:\n",
    "\n",
    "```bash\n",
    "# Instala Ollama (solo una vez)\n",
    "wget -qO- https://ollama.com/install.sh | sh\n",
    "# Arranca el servidor\n",
    "ollama serve &\n",
    "# Descarga y prepara el modelo (puede tardar unos minutos)\n",
    "ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "En **LM Studio** puedes descargar modelos desde la interfaz gr√°fica y exponer un endpoint local. Una vez en marcha, LangChain se conecta mediante la clase `ChatOllama` indicando el nombre del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2698f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el modelo local (aseg√∫rate de que el servidor de Ollama est√© ejecut√°ndose)\n",
    "# Si utilizas LM Studio, cambia el nombre del modelo o ajusta el endpoint.\n",
    "llm = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Tambi√©n puedes ajustar par√°metros como temperatura, top_p, etc.\n",
    "# llm = ChatOllama(model=\"llama3:8b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38857",
   "metadata": {},
   "source": [
    "## 3. Crear memoria y construir el agente\n",
    "\n",
    "La memoria mantiene el historial de conversaci√≥n. Usaremos `ConversationBufferMemory` para recordar los mensajes pasados. Luego construiremos un **agente reactivo** con `create_react_agent`, pasando el modelo, la lista de herramientas y la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una memoria de conversaci√≥n\n",
    "memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Crear el prompt que usar√° el agente\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til que puede usar herramientas para completar tareas.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente reactivo (ReAct)\n",
    "agente = create_react_agent(llm, herramientas, prompt)\n",
    "\n",
    "# Ejecutar el agente dentro de un executor para gestionar el estado y la memoria\n",
    "executor = AgentExecutor(agent=agente, tools=herramientas, memory=memoria, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb9f",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el agente\n",
    "\n",
    "Ya podemos hacer consultas al agente. El agente decidir√° si necesita llamar a alguna herramienta para calcular o buscar informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta: combinaci√≥n de c√°lculo\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¬øCu√°nto es 5*7 y 3*8?\"}]}\n",
    "# Para probar el agente descomenta las l√≠neas siguientes cuando tengas el modelo local ejecut√°ndose:\n",
    "respuesta = executor.invoke(pregunta)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62883c8f",
   "metadata": {},
   "source": [
    "## 5. Ejemplo de agente calculadora\n",
    "\n",
    "Construiremos un agente que s√≥lo utiliza herramientas aritm√©ticas para resolver preguntas de c√°lculo. El agente decidir√° si debe llamar a la funci√≥n de suma o multiplicaci√≥n en funci√≥n de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuevamente nuestras herramientas aritm√©ticas\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    return a * b\n",
    "\n",
    "# Lista de herramientas\n",
    "calculadora_tools = [sumar, multiplicar]\n",
    "\n",
    "# Instanciamos un modelo local (por ejemplo, llama3)\n",
    "llm_calc = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Memoria para el agente\n",
    "mem_calc = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt base\n",
    "prompt_calc = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres una calculadora que puede usar herramientas para sumar y multiplicar.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear agente y executor\n",
    "agente_calc = create_react_agent(llm_calc, calculadora_tools, prompt_calc)\n",
    "executor_calc = AgentExecutor(agent=agente_calc, tools=calculadora_tools, memory=mem_calc, verbose=True)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¬øCu√°nto es 4*6 m√°s 10?\"}]}\n",
    "# respuesta = executor_calc.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693985",
   "metadata": {},
   "source": [
    "## 6. Agente de b√∫squeda de art√≠culos cient√≠ficos con Playwright, Gemini y Ollama\n",
    "\n",
    "En este ejemplo final combinamos la automatizaci√≥n web mediante **Playwright** con dos modelos de lenguaje distintos. El agente navega a una base de datos de art√≠culos (por ejemplo Google Scholar), realiza una b√∫squeda de art√≠culos cient√≠ficos sobre un tema y extrae los t√≠tulos. Luego utiliza un modelo de **Gemini** para filtrar y evaluar la relevancia de los resultados, y finalmente emplea un modelo local de **Ollama** para sintetizar una respuesta para el usuario.\n",
    "\n",
    "**Nota:** Playwright necesita instalarse (`pip install playwright` y luego `playwright install`), y el acceso a Gemini requiere configurar credenciales de Google generative AI. Este ejemplo es ilustrativo y no se ejecutar√° en este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de agente que usa Playwright para buscar art√≠culos y combina dos modelos\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "@tool\n",
    "def buscar_articulos(topic: str) -> str:\n",
    "    \"\"\"Busca art√≠culos cient√≠ficos sobre un tema utilizando Playwright y devuelve los t√≠tulos encontrados.\"\"\"\n",
    "    from playwright.sync_api import sync_playwright\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        # Navegar a Google Scholar\n",
    "        page.goto(\"https://scholar.google.com\")\n",
    "        page.fill(\"input[name=q]\", topic)\n",
    "        page.press(\"input[name=q]\", \"Enter\")\n",
    "        page.wait_for_selector(\"h3\")\n",
    "        # Extraer los primeros 5 t√≠tulos\n",
    "        titles = page.eval_on_selector_all(\"h3\", \"elements => elements.slice(0,5).map(e => e.innerText)\")\n",
    "        browser.close()\n",
    "        return \"\".join(titles)\n",
    "\n",
    "# Instanciamos los modelos\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "llm_ollama = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Lista de herramientas\n",
    "tools_playwright = [buscar_articulos]\n",
    "\n",
    "# Prompt base para el agente\n",
    "prompt_playwright = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente investigador que puede usar herramientas para buscar y evaluar art√≠culos.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente que decide con Gemini\n",
    "agente_playwright = create_react_agent(llm_gemini, tools_playwright, prompt_playwright)\n",
    "\n",
    "# Executor que sintetiza la respuesta final usando Ollama\n",
    "mem_playwright = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "executor_playwright = AgentExecutor(agent=agente_playwright, tools=tools_playwright, memory=mem_playwright, verbose=True, llm=llm_ollama)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"Busca art√≠culos sobre inteligencia artificial explicable.\"}]}\n",
    "# respuesta = executor_playwright.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
