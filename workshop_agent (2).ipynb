{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac40623",
   "metadata": {},
   "source": [
    "# Workshop: De cero a Agente con LangChain y Python\n",
    "\n",
    "En este workshop de 4 horas aprenderás a construir un agente inteligente desde cero utilizando **LangChain**, **Python** y modelos de lenguaje **open source** como **Ollama** o **LM Studio**. A lo largo del taller veremos los conceptos básicos de los LLMs, cómo orquestar herramientas y memorias con LangChain, y cómo integrar un vector store para recuperación aumentada de datos (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8227ed",
   "metadata": {},
   "source": [
    "## Agenda del Workshop\n",
    "\n",
    "1. Introducción a los LLMs y la ejecución local\n",
    "2. Componentes de LangChain (modelos, chains, herramientas)\n",
    "3. Definición de herramientas personalizadas\n",
    "4. Configuración de un modelo local (Ollama / LM Studio)\n",
    "5. Creación de un agente reactivo con memoria\n",
    "6. Integración con un vector store para RAG\n",
    "7. Demostración final y conclusiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4245e",
   "metadata": {},
   "source": [
    "## Conexión directa a un LLM sin LangChain\n",
    "\n",
    "Antes de introducir LangChain, veamos cómo podríamos interactuar con un modelo de lenguaje utilizando únicamente el SDK o la API que proporciona el modelo. Por ejemplo, un servidor de Ollama expone un endpoint HTTP `http://localhost:11434/api/generate` donde puedes enviar un prompt y recibir la respuesta del modelo en formato JSON. De forma análoga, otros proveedores (como OpenAI) ofrecen SDKs o endpoints REST para invocar sus modelos.\n",
    "\n",
    "Interactuar de forma directa es útil para pruebas sencillas, pero pronto verás que gestionar memoria de conversación, combinar varios modelos, reintentar peticiones o integrar fuentes de datos externas se vuelve complejo. Aquí es donde entra LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35212730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de llamada directa a un modelo local de Ollama\n",
    "import requests\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"llama3:8b\",\n",
    "    \"prompt\": \"Hola, ¿cómo estás?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petición POST al endpoint de Ollama\n",
    "# (Nota: esta llamada sólo funcionará si tienes ollama corriendo de forma local)\n",
    "# response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "# print(response.json())\n",
    "\n",
    "# Con otros proveedores usarías sus SDK o un endpoint similar. Aquí simplemente mostramos el cuerpo de la solicitud.\n",
    "\n",
    "# Ejemplo de llamada a un modelo de OpenAI\n",
    "import openai\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"prompt\": \"Hola, ¿cómo estás?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petición POST al endpoint de OpenAI\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hola, ¿cómo estás?\"}]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# ejemplo de llamado a google gemini\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"gemini-2.0-flash\",\n",
    "    \"prompt\": \"Hola, ¿cómo estás?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petición POST al endpoint de Google Gemini\n",
    "response = genai.generate_content(data)\n",
    "print(response.text)\n",
    "\n",
    "# ejemplo de llamado a openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b51ae",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Por qué utilizar LangChain?\n",
    "\n",
    "Aunque podrías interactuar directamente con un modelo de lenguaje usando el SDK que ofrece cada proveedor (por ejemplo la API de **OpenAI**, el servidor local de **Ollama** o el endpoint de **LM Studio**), **LangChain** proporciona una capa de abstracción y orquestación muy útil cuando necesitas construir agentes más sofisticados:\n",
    "\n",
    "- **Unifica interfaces**: te permite cambiar entre distintos LLMs (open source o propietarios) sin modificar el resto de tu código, porque expone una API común para modelos de chat, embeddings y vector stores.\n",
    "- **Encadena tareas**: facilita construir *chains* donde la salida de una llamada se usa como entrada de otra, incluyendo flujos de preguntas y respuestas, análisis de datos o ejecución de herramientas externas.\n",
    "- **Gestión de memoria**: ofrece componentes para almacenar el historial de conversaciones y recuperarlo, algo esencial para agentes conversacionales.\n",
    "- **Integración de herramientas**: permite exponer funciones personalizadas (cálculos, búsquedas, consultas API, etc.) como herramientas que el modelo puede invocar cuando es necesario.\n",
    "- **Recuperación aumentada (RAG)**: se integra con motores de vector y sistemas de embeddings para buscar documentos relevantes y combinarlos con la generación del LLM.\n",
    "\n",
    "En resumen, LangChain actúa como el pegamento que conecta los diferentes bloques (modelos, herramientas, bases de datos) y te permite centrarte en la lógica de tu agente en lugar de los detalles de cada SDK.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3af0",
   "metadata": {},
   "source": [
    "### Instalación y configuración\n",
    "\n",
    "Para seguir este notebook necesitas instalar varias librerías. Si ya tienes un entorno con `langchain` y `chromadb` puedes omitir esta celda. En una máquina local con acceso a internet se pueden instalar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c8229",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-community langchain-core chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a185c",
   "metadata": {},
   "source": [
    "### Importación de módulos\n",
    "\n",
    "Importamos las clases y funciones necesarias para construir el agente. Esto incluye el modelo local (por ejemplo `ChatOllama`), el motor de memoria, las herramientas y funciones auxiliares de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool, tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861e6ec",
   "metadata": {},
   "source": [
    "## Cadenas en LangChain\n",
    "\n",
    "Una **cadena** combina uno o más componentes (prompts, modelos, transformaciones) para construir un flujo de ejecución. LangChain incluye utilidades como `LLMChain` para encapsular un prompt y un modelo. Aquí tienes un ejemplo de cadena simple que genera una respuesta a partir de un template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cadena simple\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definimos un prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Dime un dato curioso sobre {tema}.\")\n",
    "\n",
    "# Instanciamos el modelo local (suponiendo que esté en marcha)\n",
    "llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Creamos la cadena\n",
    "cadena = llm_local | prompt\n",
    "\n",
    "# Para ejecutarla proporcionaríamos las variables del template:\n",
    "resultado = cadena.invoke({\"tema\": \"Colombia\"})\n",
    "# print(resultado)\n",
    "# Nota: descomenta estas líneas para ejecutar con un modelo local en marcha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b09f95",
   "metadata": {},
   "source": [
    "## Memoria en LangChain\n",
    "\n",
    "Los agentes conversacionales necesitan recordar lo que ya se ha dicho. LangChain ofrece varias implementaciones de memoria, como `ConversationBufferMemory`, que almacena el historial de mensajes en orden. Puedes combinarla con un LLMChain o un agente para que el modelo reciba contexto en cada llamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de ConversationBufferMemory con un LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Creamos la memoria\n",
    "memoria_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Definimos el prompt\n",
    "prompt_memoria = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente amistoso.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{chat_history}\"),\n",
    "])\n",
    "\n",
    "# Creamos la cadena con memoria\n",
    "cadena_memoria = LLMChain(llm=llm_local, prompt=prompt_memoria, memory=memoria_chain)\n",
    "\n",
    "# Para usarla, invoca la cadena varias veces; la memoria conservará el historial:\n",
    "# respuesta1 = cadena_memoria.invoke({\"input\": \"Hola\"})\n",
    "# respuesta2 = cadena_memoria.invoke({\"input\": \"¿Qué me dijiste antes?\"})\n",
    "# print(respuesta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8351375",
   "metadata": {},
   "source": [
    "## Plantillas de Prompt (Prompt Templates)\n",
    "\n",
    "LangChain facilita la construcción de prompts complejos mediante plantillas parametrizadas. Puedes combinar mensajes de sistema, de usuario y del asistente para definir el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba42c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de plantilla de prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "plantilla = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en matemáticas.\"),\n",
    "    (\"human\", \"Pregunta: {pregunta}\"),\n",
    "])\n",
    "\n",
    "# Instanciaríamos el modelo y llamaríamos:\n",
    "# llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "# chain = LLMChain(llm=llm_local, prompt=plantilla)\n",
    "# respuesta = chain.invoke({\"pregunta\": \"¿Cuánto es 12×8?\"})\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172daf6a",
   "metadata": {},
   "source": [
    "## Utilizar diferentes modelos con la misma lógica\n",
    "\n",
    "Gracias a las capas de abstracción de LangChain, puedes cambiar de un modelo local (por ejemplo, vía `ChatOllama`) a un modelo alojado (por ejemplo, `ChatOpenAI`) sin reescribir el flujo de tu aplicación. Sólo debes cambiar la clase del modelo cuando lo instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de inicialización con un modelo de OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Suponiendo que tengas configurada la API Key en tu entorno\n",
    "# openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "# La cadena o el agente podrían usar indistintamente openai_llm en lugar de llm_local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906b3f",
   "metadata": {},
   "source": [
    "## 1. Definir herramientas personalizadas\n",
    "\n",
    "Las **herramientas** permiten que el agente ejecute funciones específicas (por ejemplo cálculos o búsquedas). Definiremos dos funciones sencillas que suman y multiplican números. Utilizamos el decorador `@tool` para convertirlas en herramientas que LangChain pueda invocar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    # Suma dos números y devuelve el resultado\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    # Multiplica dos números y devuelve el resultado\n",
    "    return a * b\n",
    "\n",
    "# Registrar las herramientas en una lista\n",
    "herramientas = [sumar, multiplicar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696df71",
   "metadata": {},
   "source": [
    "## 2. Instanciar el modelo local\n",
    "\n",
    "Antes de crear el agente necesitamos un LLM local en ejecución. Con **Ollama** podemos iniciar un servidor y cargar un modelo como *llama3:8b*:\n",
    "\n",
    "```bash\n",
    "# Instala Ollama (solo una vez)\n",
    "wget -qO- https://ollama.com/install.sh | sh\n",
    "# Arranca el servidor\n",
    "ollama serve &\n",
    "# Descarga y prepara el modelo (puede tardar unos minutos)\n",
    "ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "En **LM Studio** puedes descargar modelos desde la interfaz gráfica y exponer un endpoint local. Una vez en marcha, LangChain se conecta mediante la clase `ChatOllama` indicando el nombre del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2698f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el modelo local (asegúrate de que el servidor de Ollama esté ejecutándose)\n",
    "# Si utilizas LM Studio, cambia el nombre del modelo o ajusta el endpoint.\n",
    "llm = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# También puedes ajustar parámetros como temperatura, top_p, etc.\n",
    "# llm = ChatOllama(model=\"llama3:8b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38857",
   "metadata": {},
   "source": [
    "## 3. Crear memoria y construir el agente\n",
    "\n",
    "La memoria mantiene el historial de conversación. Usaremos `ConversationBufferMemory` para recordar los mensajes pasados. Luego construiremos un **agente reactivo** con `create_react_agent`, pasando el modelo, la lista de herramientas y la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una memoria de conversación\n",
    "memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Crear el prompt que usará el agente\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil que puede usar herramientas para completar tareas.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente reactivo (ReAct)\n",
    "agente = create_react_agent(llm, herramientas, prompt)\n",
    "\n",
    "# Ejecutar el agente dentro de un executor para gestionar el estado y la memoria\n",
    "executor = AgentExecutor(agent=agente, tools=herramientas, memory=memoria, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb9f",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el agente\n",
    "\n",
    "Ya podemos hacer consultas al agente. El agente decidirá si necesita llamar a alguna herramienta para calcular o buscar información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta: combinación de cálculo\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuánto es 5*7 y 3*8?\"}]}\n",
    "# Para probar el agente descomenta las líneas siguientes cuando tengas el modelo local ejecutándose:\n",
    "# respuesta = executor.invoke(pregunta)\n",
    "# print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770afb66",
   "metadata": {},
   "source": [
    "## 5. Integrar un vector store para Recuperación Aumentada (RAG)\n",
    "\n",
    "Para responder preguntas sobre tus propios documentos, podemos crear una herramienta de recuperación de texto basada en embeddings. Utilizaremos **Chroma** como vector store y **HuggingFaceEmbeddings** como modelo de embedding.\n",
    "\n",
    "Primero cargamos algunos textos de ejemplo y construimos el vector store. Luego exponemos una herramienta `search_docs` que busca los documentos más relevantes y devuelve su contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7100d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "#\n",
    "# textos = [\n",
    "#     \"Colombia es un país situado en América del Sur cuya capital es Bogotá.\",\n",
    "#     \"La multiplicación es una operación matemática que suma un número consigo mismo muchas veces.\",\n",
    "# ]\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# store = Chroma.from_texts(textos, embeddings)\n",
    "# retriever = store.as_retriever()\n",
    "#\n",
    "# @tool\n",
    "# def search_docs(query: str) -> str:\n",
    "#     # Busca documentos relevantes y devuelve el texto más parecido\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "#     return docs[0].page_content\n",
    "#\n",
    "# # Añadimos esta herramienta a la lista de herramientas del agente\n",
    "# herramientas.append(search_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af084e8f",
   "metadata": {},
   "source": [
    "## 6. Demostración final\n",
    "\n",
    "Después de añadir la herramienta de recuperación, el agente puede combinar cálculo y búsqueda de datos. Por ejemplo:\n",
    "\n",
    "```python\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuál es la capital de Colombia y cuánto es 9×7?\"}]}\n",
    "# respuesta = executor.invoke(pregunta)\n",
    "# print(respuesta)\n",
    "```\n",
    "\n",
    "El agente consultará la herramienta de búsqueda para obtener la capital (Bogotá) y la herramienta de multiplicación para calcular 9×7, devolviendo una respuesta completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67cc7b",
   "metadata": {},
   "source": [
    "## 7. Conclusiones y próximos pasos\n",
    "\n",
    "En este notebook has visto cómo:\n",
    "\n",
    "- Instalar y configurar las dependencias necesarias para ejecutar modelos locales.\n",
    "- Definir herramientas personalizadas con el decorador `@tool`.\n",
    "- Inicializar un modelo local utilizando **Ollama** o **LM Studio**.\n",
    "- Construir un agente reactivo con memoria y herramientas en LangChain.\n",
    "- Integrar un vector store para realizar búsquedas en tus documentos (RAG).\n",
    "\n",
    "**Próximos pasos:** explora otros modelos open source (Mistral, Mixtral), añade nuevas herramientas (APIs, bases de datos) y optimiza el flujo de tu agente con LangGraph."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
