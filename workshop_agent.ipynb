{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac40623",
   "metadata": {},
   "source": [
    "# Workshop: De cero a Agente con LangChain y Python\n",
    "\n",
    "En este workshop de 4 horas aprenderás a construir un agente inteligente desde cero utilizando **LangChain**, **Python** y modelos de lenguaje **open source** como **Ollama** o **LM Studio**. A lo largo del taller veremos los conceptos básicos de los LLMs, cómo orquestar herramientas y memorias con LangChain, y cómo integrar un vector store para recuperación aumentada de datos (RAG)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8227ed",
   "metadata": {},
   "source": [
    "## Agenda del Workshop\n",
    "\n",
    "1. Introducción a los LLMs y la ejecución local\n",
    "2. Componentes de LangChain (modelos, chains, herramientas)\n",
    "3. Definición de herramientas personalizadas\n",
    "4. Configuración de un modelo local (Ollama / LM Studio)\n",
    "5. Creación de un agente reactivo con memoria\n",
    "6. Integración con un vector store para RAG\n",
    "7. Demostración final y conclusiones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3af0",
   "metadata": {},
   "source": [
    "### Instalación y configuración\n",
    "\n",
    "Para seguir este notebook necesitas instalar varias librerías. Si ya tienes un entorno con `langchain` y `chromadb` puedes omitir esta celda. En una máquina local con acceso a internet se pueden instalar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c8229",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-community langchain-core chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a185c",
   "metadata": {},
   "source": [
    "### Importación de módulos\n",
    "\n",
    "Importamos las clases y funciones necesarias para construir el agente. Esto incluye el modelo local (por ejemplo `ChatOllama`), el motor de memoria, las herramientas y funciones auxiliares de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool, tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906b3f",
   "metadata": {},
   "source": [
    "## 1. Definir herramientas personalizadas\n",
    "\n",
    "Las **herramientas** permiten que el agente ejecute funciones específicas (por ejemplo cálculos o búsquedas). Definiremos dos funciones sencillas que suman y multiplican números. Utilizamos el decorador `@tool` para convertirlas en herramientas que LangChain pueda invocar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    # Suma dos números y devuelve el resultado\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    # Multiplica dos números y devuelve el resultado\n",
    "    return a * b\n",
    "\n",
    "# Registrar las herramientas en una lista\n",
    "herramientas = [sumar, multiplicar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696df71",
   "metadata": {},
   "source": [
    "## 2. Instanciar el modelo local\n",
    "\n",
    "Antes de crear el agente necesitamos un LLM local en ejecución. Con **Ollama** podemos iniciar un servidor y cargar un modelo como *llama3:8b*:\n",
    "\n",
    "```bash\n",
    "# Instala Ollama (solo una vez)\n",
    "wget -qO- https://ollama.com/install.sh | sh\n",
    "# Arranca el servidor\n",
    "ollama serve &\n",
    "# Descarga y prepara el modelo (puede tardar unos minutos)\n",
    "ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "En **LM Studio** puedes descargar modelos desde la interfaz gráfica y exponer un endpoint local. Una vez en marcha, LangChain se conecta mediante la clase `ChatOllama` indicando el nombre del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2698f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el modelo local (asegúrate de que el servidor de Ollama esté ejecutándose)\n",
    "# Si utilizas LM Studio, cambia el nombre del modelo o ajusta el endpoint.\n",
    "llm = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# También puedes ajustar parámetros como temperatura, top_p, etc.\n",
    "# llm = ChatOllama(model=\"llama3:8b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38857",
   "metadata": {},
   "source": [
    "## 3. Crear memoria y construir el agente\n",
    "\n",
    "La memoria mantiene el historial de conversación. Usaremos `ConversationBufferMemory` para recordar los mensajes pasados. Luego construiremos un **agente reactivo** con `create_react_agent`, pasando el modelo, la lista de herramientas y la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una memoria de conversación\n",
    "memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Crear el prompt que usará el agente\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil que puede usar herramientas para completar tareas.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente reactivo (ReAct)\n",
    "agente = create_react_agent(llm, herramientas, prompt)\n",
    "\n",
    "# Ejecutar el agente dentro de un executor para gestionar el estado y la memoria\n",
    "executor = AgentExecutor(agent=agente, tools=herramientas, memory=memoria, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb9f",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el agente\n",
    "\n",
    "Ya podemos hacer consultas al agente. El agente decidirá si necesita llamar a alguna herramienta para calcular o buscar información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta: combinación de cálculo\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuánto es 5*7 y 3*8?\"}]}\n",
    "# Para probar el agente descomenta las líneas siguientes cuando tengas el modelo local ejecutándose:\n",
    "# respuesta = executor.invoke(pregunta)\n",
    "# print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770afb66",
   "metadata": {},
   "source": [
    "## 5. Integrar un vector store para Recuperación Aumentada (RAG)\n",
    "\n",
    "Para responder preguntas sobre tus propios documentos, podemos crear una herramienta de recuperación de texto basada en embeddings. Utilizaremos **Chroma** como vector store y **HuggingFaceEmbeddings** como modelo de embedding.\n",
    "\n",
    "Primero cargamos algunos textos de ejemplo y construimos el vector store. Luego exponemos una herramienta `search_docs` que busca los documentos más relevantes y devuelve su contenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7100d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "#\n",
    "# textos = [\n",
    "#     \"Colombia es un país situado en América del Sur cuya capital es Bogotá.\",\n",
    "#     \"La multiplicación es una operación matemática que suma un número consigo mismo muchas veces.\",\n",
    "# ]\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# store = Chroma.from_texts(textos, embeddings)\n",
    "# retriever = store.as_retriever()\n",
    "#\n",
    "# @tool\n",
    "# def search_docs(query: str) -> str:\n",
    "#     # Busca documentos relevantes y devuelve el texto más parecido\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "#     return docs[0].page_content\n",
    "#\n",
    "# # Añadimos esta herramienta a la lista de herramientas del agente\n",
    "# herramientas.append(search_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af084e8f",
   "metadata": {},
   "source": [
    "## 6. Demostración final\n",
    "\n",
    "Después de añadir la herramienta de recuperación, el agente puede combinar cálculo y búsqueda de datos. Por ejemplo:\n",
    "\n",
    "```python\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuál es la capital de Colombia y cuánto es 9×7?\"}]}\n",
    "# respuesta = executor.invoke(pregunta)\n",
    "# print(respuesta)\n",
    "```\n",
    "\n",
    "El agente consultará la herramienta de búsqueda para obtener la capital (Bogotá) y la herramienta de multiplicación para calcular 9×7, devolviendo una respuesta completa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67cc7b",
   "metadata": {},
   "source": [
    "## 7. Conclusiones y próximos pasos\n",
    "\n",
    "En este notebook has visto cómo:\n",
    "\n",
    "- Instalar y configurar las dependencias necesarias para ejecutar modelos locales.\n",
    "- Definir herramientas personalizadas con el decorador `@tool`.\n",
    "- Inicializar un modelo local utilizando **Ollama** o **LM Studio**.\n",
    "- Construir un agente reactivo con memoria y herramientas en LangChain.\n",
    "- Integrar un vector store para realizar búsquedas en tus documentos (RAG).\n",
    "\n",
    "**Próximos pasos:** explora otros modelos open source (Mistral, Mixtral), añade nuevas herramientas (APIs, bases de datos) y optimiza el flujo de tu agente con LangGraph."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
