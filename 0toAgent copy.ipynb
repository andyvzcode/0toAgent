{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac40623",
   "metadata": {},
   "source": [
    "# Workshop: De cero a Agente con LangChain y Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4245e",
   "metadata": {},
   "source": [
    "## Conexi√≥n directa a un LLM sin LangChain\n",
    "\n",
    "Antes de introducir LangChain, veamos c√≥mo podr√≠amos interactuar con un modelo de lenguaje utilizando √∫nicamente el SDK o la API que proporciona el modelo. Por ejemplo, un servidor de Ollama expone un endpoint HTTP `http://localhost:11434/api/generate` donde puedes enviar un prompt y recibir la respuesta del modelo en formato JSON. De forma an√°loga, otros proveedores (como OpenAI) ofrecen SDKs o endpoints REST para invocar sus modelos.\n",
    "\n",
    "Interactuar de forma directa es √∫til para pruebas sencillas, pero pronto ver√°s que gestionar memoria de conversaci√≥n, combinar varios modelos, reintentar peticiones o integrar fuentes de datos externas se vuelve complejo. Aqu√≠ es donde entra LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46316392",
   "metadata": {},
   "source": [
    "# Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c6699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "LangChain es un proyecto de inteligencia artificial basado en el modelo deep learning que se centra en la generaci√≥n autom√°tica de textos. Seg√∫n lo conocido, el modelo, llamado LLaMA (Large Language Model), es un redujo computacional que capaz(es) a las personas no programadas para lidiar con texto (en espa√±ol) y a trav√©s de procesos de inteligencia artificial (IA) al intentar entender, Analizar e interpretar datos. \n",
      "\n",
      "El langchain se centra en la generaci√≥n autom√°tica de texto, lo que significa que el modelo puede create textos nunca vistos o entendidos como si estuviera bien adentuw. Puedes usar el langchain para tasks como traducir texto a espa√±ol o para creer texto personalized basado en datos.\n",
      "\n",
      "Sin embargo, es importante notar que el langchain no himself es unaÂèòÂΩ¢ ou veritable de la IA; su objetivo principal es solely focused en la generaci√≥n autom√°tica de textos.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de llamada directa a un modelo local de Ollama\n",
    "import requests\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"prompt\": \"Que es langchain?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petici√≥n POST al endpoint de Ollama\n",
    "# (Nota: esta llamada s√≥lo funcionar√° si tienes ollama corriendo de forma local)\n",
    "response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "print(response.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0303e",
   "metadata": {},
   "source": [
    "# OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1d789a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (1.75.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b5cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain es un *framework de c√≥digo abierto* para desarrollar aplicaciones impulsadas por modelos de lenguaje (LLMs) como GPT, Claude, Llama, etc.** Su objetivo principal es **simplificar la creaci√≥n de aplicaciones complejas que combinan LLMs con otras fuentes de datos o servicios, y gestionar sus interacciones de manera flexible y potente.**\n",
      "\n",
      "Piensa en LangChain como un conjunto de \"piezas de LEGO\" estandarizadas que te permiten:\n",
      "\n",
      "1.  **Conectar LLMs** f√°cilmente (OpenAI, Anthropic, Hugging Face, etc.).\n",
      "2.  **Integrar Datos Externos** (APIs, bases de datos, documentos PDF, hojas de c√°lculo, sitios web).\n",
      "3.  **Secuenciar Llamadas** (\"encadenar\" pasos donde la salida de un LLM se convierte en la entrada del siguiente o de una acci√≥n externa).\n",
      "4.  **Manejar el Estado** (recordar el historial de la conversaci√≥n, contexto).\n",
      "5.  **Incorporar Memoria** (para aplicaciones conversacionales).\n",
      "\n",
      "### ¬øPor qu√© lo necesitas?\n",
      "Usar directamente la API de un LLM para una pregunta sencilla est√° bien (`modelo.generate(\"¬øQu√© es el sol?\")`). Pero para aplicaciones reales necesitas:\n",
      "\n",
      "*   **Consultar documentos personales o bases de datos:** \"Resume este informe PDF para mi jefe\".\n",
      "*   **Tomar decisiones secuenciales:** \"Analiza estos datos, saca conclusiones y luego escribe un correo con ellas\".\n",
      "*   **Interactuar con herramientas:** \"Busca el clima actual en Buenos Aires y sugiere qu√© ropa llevar\".\n",
      "*   **Mantener conversaciones coherentes:** Recordar lo dicho antes en un chat.\n",
      "\n",
      "**LangChain proporciona los componentes (\"abstracciones\") para hacer esto de forma estructurada y reutilizable.**\n",
      "\n",
      "### Conceptos Clave (Componentes)\n",
      "\n",
      "1.  **Models (Modelos):** La capa de los LLMs (OpenAI GPT, Anthropic Claude, etc.) o modelos incrustados (Embeddings).\n",
      "2.  **Prompts (Indicaciones):** Plantillas para estructurar y optimizar la entrada al modelo (prompt engineering).\n",
      "3.  **Chains (Cadenas):** **El n√∫cleo**. Combinaciones de llamadas a modelos, l√≥gica y otros componentes en una secuencia. Pueden ser simples (Prompt -> LLM) o complejas.\n",
      "    *   **LCEL (LangChain Expression Language):** Sintaxis declarativa para construir cadenas complejas de forma fluida.\n",
      "4.  **Agents (Agentes):** Componentes que permiten al LLM **tomar decisiones** din√°micas sobre qu√© herramienta usar y en qu√© orden para resolver una tarea compleja. El \"cerebro\" que orquesta.\n",
      "5.  **Tools (Herramientas):** Funciones que el Agente puede llamar (buscar en Google, consultar una API, una calculadora, etc.).\n",
      "6.  **Memory (Memoria):** Mecanismo para persistir el estado/conversaci√≥n entre interacciones con el modelo.\n",
      "7.  **Retrieval (Recuperaci√≥n):** Para conectar grandes vol√∫menes de datos propios con el LLM, clave para RAG (\"Retrieval-Augmented Generation\"). Incluye embeddings y bases de datos vectoriales.\n",
      "8.  **Output Parsers (Parsers de Salida):** Convierten la salida desestructurada del LLM (texto) en un formato estructurado (JSON, lista, etc.).\n",
      "\n",
      "### Analog√≠a Simple\n",
      "Imagina que construir una app con LLM es como construir una f√°brica:\n",
      "\n",
      "*   **Los LLMs** son trabajadores inteligentes pero ingenuos.\n",
      "*   **Las Tools** son m√°quinas especializadas (impresoras, cortadoras).\n",
      "*   **Los Agents** son los capataces que deciden qu√© trabajador hace qu√© tarea y qu√© m√°quina usar seg√∫n las √≥rdenes.\n",
      "*   **La Memory** son los registros de producci√≥n pasada.\n",
      "*   **La Retrieval** es el almac√©n de materias primas propias.\n",
      "*   **Los Chains** son las l√≠neas de montaje (pasos fijos).\n",
      "*   **LangChain** es el kit de construcci√≥n estandarizado y los planos para dise√±ar toda la f√°brica.\n",
      "\n",
      "### Ejemplos de Aplicaciones (Lo que puedes construir)\n",
      "\n",
      "*   **Chatbots sofisticados:** Que consulten bases de conocimientos internas (RAG).\n",
      "*   **Agentes Anal√≠ticos:** Que analicen datos, generen reportes, alerten.\n",
      "*   **Asistentes de C√≥digo:** Que entiendan y modifiquen un repositorio entero.\n",
      "*   **Resumidores de Documentaci√≥n:** Personalizados.\n",
      "*   **Interfaces de conversaci√≥n con datos internos:** Pregunta a tu base de datos con lenguaje natural.\n",
      "*   **Automatizaci√≥n de flujos de trabajo:** Generaci√≥n de contenido, clasificaci√≥n, extracci√≥n de datos + toma de acciones posteriores.\n",
      "\n",
      "### Ejemplo m√≠nimo de C√≥digo (Python)\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "\n",
      "# 1. Modelo\n",
      "llm = ChatOpenAI(api_key=\"tu_api_key\", model=\"gpt-3.5-turbo\")\n",
      "\n",
      "# 2. Prompt (Plantilla)\n",
      "prompt = ChatPromptTemplate.from_template(\n",
      "    \"Traduce al {lenguaje} el siguiente texto: {texto}\"\n",
      ")\n",
      "\n",
      "# 3. Chain SIMPLE (Prompt -> LLM)\n",
      "chain = prompt | llm  # LCEL: Usa el operador pipe '|'\n",
      "\n",
      "# Ejecutar\n",
      "respuesta = chain.invoke({\n",
      "    \"lenguaje\": \"franc√©s\",\n",
      "    \"texto\": \"¬°Hola! ¬øC√≥mo est√°s?\"\n",
      "})\n",
      "print(respuesta.content)  # Salida: \"Bonjour ! Comment √ßa va ?\"\n",
      "```\n",
      "\n",
      "**En resumen:** LangChain es el *framework imprescindible* para pasar de hacer preguntas puntuales a LLMs a construir aplicaciones robustas, integradas y aut√≥nomas que aprovechen todo su potencial combinado con tus datos y sistemas. Es la capa de orquestaci√≥n y abstracci√≥n sobre los LLMs.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"MI PAGINA o APP\", \n",
    "    \"X-Title\": \"ANDY CODE\",\n",
    "  },\n",
    "  model=\"deepseek/deepseek-r1-0528:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Que es langchain?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14f5ab",
   "metadata": {},
   "source": [
    "# Google SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0134f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e164470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It learns patterns from data to make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15c64",
   "metadata": {},
   "source": [
    "# Tools (Herramientas) Personalizadas con Google Gemini\n",
    "\n",
    "Ahora vamos a aprender a crear herramientas personalizadas que los modelos pueden usar. Utilizaremos Google Gemini como nuestro modelo principal para todos los ejemplos.\n",
    "\n",
    "## ¬øQu√© son las Tools?\n",
    "\n",
    "üîß **Funciones** que los LLMs pueden invocar para realizar tareas espec√≠ficas  \n",
    "üåê **Extensiones** de las capacidades b√°sicas del modelo  \n",
    "‚ö° **Automatizaci√≥n** de procesos complejos  \n",
    "üéØ **Integraci√≥n** con sistemas externos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9863301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Google Gemini configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n inicial con Google Gemini\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configurar Google API\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDKPdCv74mFw9TsjWnqjWLTazlBSIncocs\"\n",
    "\n",
    "# Crear instancia del modelo Gemini\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Google Gemini configurado correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4827",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Herramientas Matem√°ticas B√°sicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d47c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRUEBAS DE HERRAMIENTAS ===\n",
      "Resultado: 15 + 25 = 40\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for calcular_area_circulo\n  Input should be a valid dictionary or instance of calcular_area_circulo [type=model_type, input_value=5, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== PRUEBAS DE HERRAMIENTAS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(calculadora_basica.invoke(\u001b[33m\"\u001b[39m\u001b[33m15 + 25\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcalcular_area_circulo\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(convertir_temperatura.invoke(\u001b[32m25\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(obtener_fecha_hora.invoke(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:513\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs: Any,\n\u001b[32m    511\u001b[39m ) -> Any:\n\u001b[32m    512\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:774\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    773\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    775\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    776\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:736\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    734\u001b[39m child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     tool_args, tool_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m signature(\u001b[38;5;28mself\u001b[39m._run).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    740\u001b[39m         tool_kwargs = tool_kwargs | {\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m: run_manager}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:651\u001b[39m, in \u001b[36mBaseTool._to_args_and_kwargs\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    644\u001b[39m     \u001b[38;5;28mself\u001b[39m.args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    645\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.args_schema, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m ):\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# StructuredTool with no args\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (), {}\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m tool_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;66;03m# For backwards compatibility, if run_input is a string,\u001b[39;00m\n\u001b[32m    653\u001b[39m \u001b[38;5;66;03m# pass as a positional argument.\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:570\u001b[39m, in \u001b[36mBaseTool._parse_input\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    568\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    569\u001b[39m             tool_input[k] = tool_call_id\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     result = \u001b[43minput_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     result_dict = result.model_dump()\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(input_args, BaseModelV1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/pydantic/main.py:705\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    701\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    702\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for calcular_area_circulo\n  Input should be a valid dictionary or instance of calcular_area_circulo [type=model_type, input_value=5, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "# Creamos herramientas matem√°ticas para estudiantes\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def calculadora_basica(operacion: str) -> str:\n",
    "    \"\"\"Realiza operaciones matem√°ticas b√°sicas. Formato: 'numero operador numero' (ej: '5 + 3', '10 * 2')\"\"\"\n",
    "    try:\n",
    "        # Operaciones permitidas para seguridad\n",
    "        operadores = {'+': '+', '-': '-', '*': '*', '/': '/', '^': '**'}\n",
    "        \n",
    "        # Reemplazar ^ por **\n",
    "        operacion = operacion.replace('^', '**')\n",
    "        \n",
    "        # Evaluar de forma segura\n",
    "        resultado = eval(operacion, {\"__builtins__\": {}}, {})\n",
    "        return f\"Resultado: {operacion} = {resultado}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error en el c√°lculo: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def calcular_area_circulo(radio: float) -> str:\n",
    "    \"\"\"Calcula el √°rea de un c√≠rculo dado su radio\"\"\"\n",
    "    area = math.pi * (radio ** 2)\n",
    "    return f\"El √°rea del c√≠rculo con radio {radio} es: {area:.2f} unidades cuadradas\"\n",
    "\n",
    "@tool\n",
    "def convertir_temperatura(valor: float, de_unidad: str, a_unidad: str) -> str:\n",
    "    \"\"\"Convierte temperaturas entre Celsius (C), Fahrenheit (F) y Kelvin (K)\"\"\"\n",
    "    de_unidad = de_unidad.upper()\n",
    "    a_unidad = a_unidad.upper()\n",
    "    \n",
    "    # Convertir todo a Celsius primero\n",
    "    if de_unidad == \"F\":\n",
    "        celsius = (valor - 32) * 5/9\n",
    "    elif de_unidad == \"K\":\n",
    "        celsius = valor - 273.15\n",
    "    else:\n",
    "        celsius = valor\n",
    "    \n",
    "    # Convertir de Celsius a la unidad destino\n",
    "    if a_unidad == \"F\":\n",
    "        resultado = celsius * 9/5 + 32\n",
    "    elif a_unidad == \"K\":\n",
    "        resultado = celsius + 273.15\n",
    "    else:\n",
    "        resultado = celsius\n",
    "    \n",
    "    return f\"{valor}¬∞ {de_unidad} = {resultado:.2f}¬∞ {a_unidad}\"\n",
    "\n",
    "@tool\n",
    "def obtener_fecha_hora() -> str:\n",
    "    \"\"\"Obtiene la fecha y hora actual\"\"\"\n",
    "    ahora = datetime.now()\n",
    "    return f\"Fecha y hora actual: {ahora.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Probamos las herramientas individualmente\n",
    "print(\"=== PRUEBAS DE HERRAMIENTAS ===\")\n",
    "print(calculadora_basica.invoke(\"15 + 25\"))\n",
    "print(calcular_area_circulo.invoke(5))\n",
    "print(convertir_temperatura.invoke(25, \"C\", \"F\"))\n",
    "print(obtener_fecha_hora.invoke(\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992134e",
   "metadata": {},
   "source": [
    "## Ejemplo 2: Herramientas para Estudiantes de Sistemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herramientas espec√≠ficas para programaci√≥n y sistemas\n",
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "@tool\n",
    "def generar_password(longitud: int = 12) -> str:\n",
    "    \"\"\"Genera una contrase√±a segura de la longitud especificada\"\"\"\n",
    "    import string\n",
    "    caracteres = string.ascii_letters + string.digits + \"!@#$%^&*\"\n",
    "    password = ''.join(random.choice(caracteres) for _ in range(longitud))\n",
    "    return f\"Contrase√±a generada: {password}\"\n",
    "\n",
    "@tool\n",
    "def hash_texto(texto: str, algoritmo: str = \"sha256\") -> str:\n",
    "    \"\"\"Genera el hash de un texto usando diferentes algoritmos (md5, sha1, sha256)\"\"\"\n",
    "    try:\n",
    "        if algoritmo == \"md5\":\n",
    "            hash_obj = hashlib.md5(texto.encode())\n",
    "        elif algoritmo == \"sha1\":\n",
    "            hash_obj = hashlib.sha1(texto.encode())\n",
    "        elif algoritmo == \"sha256\":\n",
    "            hash_obj = hashlib.sha256(texto.encode())\n",
    "        else:\n",
    "            return \"Algoritmo no soportado. Use: md5, sha1, sha256\"\n",
    "        \n",
    "        return f\"Hash {algoritmo} de '{texto}': {hash_obj.hexdigest()}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generando hash: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def validar_json(json_string: str) -> str:\n",
    "    \"\"\"Valida si una cadena es un JSON v√°lido\"\"\"\n",
    "    try:\n",
    "        json.loads(json_string)\n",
    "        return \"‚úÖ El JSON es v√°lido\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        return f\"‚ùå JSON inv√°lido: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def convertir_binario(numero: int, conversion: str) -> str:\n",
    "    \"\"\"Convierte n√∫meros entre decimal, binario, octal y hexadecimal\"\"\"\n",
    "    try:\n",
    "        if conversion == \"decimal_a_binario\":\n",
    "            return f\"{numero} en binario: {bin(numero)}\"\n",
    "        elif conversion == \"decimal_a_octal\":\n",
    "            return f\"{numero} en octal: {oct(numero)}\"\n",
    "        elif conversion == \"decimal_a_hex\":\n",
    "            return f\"{numero} en hexadecimal: {hex(numero)}\"\n",
    "        elif conversion == \"binario_a_decimal\":\n",
    "            decimal = int(str(numero), 2)\n",
    "            return f\"{numero} (binario) en decimal: {decimal}\"\n",
    "        else:\n",
    "            return \"Conversiones disponibles: decimal_a_binario, decimal_a_octal, decimal_a_hex, binario_a_decimal\"\n",
    "    except Exception as e:\n",
    "        return f\"Error en conversi√≥n: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def info_lenguaje_programacion(lenguaje: str) -> str:\n",
    "    \"\"\"Proporciona informaci√≥n b√°sica sobre lenguajes de programaci√≥n\"\"\"\n",
    "    lenguajes = {\n",
    "        \"python\": \"üêç Python: Lenguaje interpretado, f√°cil de aprender, usado en IA, web, automatizaci√≥n\",\n",
    "        \"javascript\": \"‚ö° JavaScript: Lenguaje para desarrollo web, tanto frontend como backend (Node.js)\",\n",
    "        \"java\": \"‚òï Java: Lenguaje compilado, orientado a objetos, usado en empresas y Android\",\n",
    "        \"c++\": \"‚öôÔ∏è C++: Lenguaje de bajo nivel, r√°pido, usado en sistemas y videojuegos\",\n",
    "        \"go\": \"üöÄ Go: Lenguaje de Google, r√°pido, usado en microservicios y sistemas distribuidos\",\n",
    "        \"rust\": \"ü¶Ä Rust: Lenguaje seguro en memoria, r√°pido, usado en sistemas y blockchain\"\n",
    "    }\n",
    "    \n",
    "    lenguaje_lower = lenguaje.lower()\n",
    "    if lenguaje_lower in lenguajes:\n",
    "        return lenguajes[lenguaje_lower]\n",
    "    else:\n",
    "        return f\"No tengo informaci√≥n sobre '{lenguaje}'. Lenguajes disponibles: {', '.join(lenguajes.keys())}\"\n",
    "\n",
    "# Probamos las herramientas de sistemas\n",
    "print(\"=== HERRAMIENTAS PARA SISTEMAS ===\")\n",
    "print(generar_password.invoke(16))\n",
    "print(hash_texto.invoke(\"Hello World\", \"sha256\"))\n",
    "print(validar_json.invoke('{\"nombre\": \"Juan\", \"edad\": 25}'))\n",
    "print(convertir_binario.invoke(42, \"decimal_a_binario\"))\n",
    "print(info_lenguaje_programacion.invoke(\"python\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e8476",
   "metadata": {},
   "source": [
    "# Agentes Sencillos con Google Gemini\n",
    "\n",
    "Ahora vamos a crear agentes que pueden tomar decisiones y usar herramientas de forma aut√≥noma. Un agente combina un LLM con herramientas y puede decidir qu√© acciones tomar.\n",
    "\n",
    "## ¬øQu√© es un Agente?\n",
    "\n",
    "ü§ñ **Sistema aut√≥nomo** que toma decisiones  \n",
    "üîß **Usa herramientas** seg√∫n sea necesario  \n",
    "üß† **Razona** sobre qu√© hacer  \n",
    "üîÑ **Itera** hasta resolver el problema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos las dependencias necesarias para agentes\n",
    "%pip install langchain-core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b29dde",
   "metadata": {},
   "source": [
    "## Agente 1: Asistente Matem√°tico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuestro primer agente simple\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Definir las herramientas matem√°ticas para el agente\n",
    "herramientas_matematicas = [\n",
    "    calculadora_basica,\n",
    "    calcular_area_circulo,\n",
    "    convertir_temperatura,\n",
    "    obtener_fecha_hora\n",
    "]\n",
    "\n",
    "# Crear el prompt para el agente matem√°tico\n",
    "prompt_matematico = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente matem√°tico especializado para estudiantes.\n",
    "    \n",
    "    CAPACIDADES:\n",
    "    - Realizar c√°lculos b√°sicos (+, -, *, /)\n",
    "    - Calcular √°reas de c√≠rculos\n",
    "    - Convertir temperaturas\n",
    "    - Obtener fecha y hora actual\n",
    "    \n",
    "    INSTRUCCIONES:\n",
    "    - Usa las herramientas disponibles cuando sea necesario\n",
    "    - Explica paso a paso los c√°lculos\n",
    "    - Si no necesitas herramientas, responde directamente\n",
    "    - S√© did√°ctico y claro en tus explicaciones\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Crear el agente\n",
    "agente_matematico = create_tool_calling_agent(\n",
    "    llm_gemini, \n",
    "    herramientas_matematicas, \n",
    "    prompt_matematico\n",
    ")\n",
    "\n",
    "# Crear el ejecutor del agente\n",
    "ejecutor_matematico = AgentExecutor(\n",
    "    agent=agente_matematico,\n",
    "    tools=herramientas_matematicas,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"üßÆ Agente Matem√°tico creado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos el agente matem√°tico\n",
    "print(\"=== PRUEBAS DEL AGENTE MATEM√ÅTICO ===\")\n",
    "\n",
    "# Pregunta 1: C√°lculo simple\n",
    "print(\"\\\\nüî¢ PREGUNTA 1:\")\n",
    "respuesta1 = ejecutor_matematico.invoke({\n",
    "    \"input\": \"Calcula el √°rea de un c√≠rculo con radio 7 metros\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta1['output']}\")\n",
    "\n",
    "# Pregunta 2: Conversi√≥n de temperatura  \n",
    "print(\"\\\\nüå°Ô∏è PREGUNTA 2:\")\n",
    "respuesta2 = ejecutor_matematico.invoke({\n",
    "    \"input\": \"Si afuera hace 85 grados Fahrenheit, ¬øcu√°ntos grados Celsius son?\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta2['output']}\")\n",
    "\n",
    "# Pregunta 3: Problema combinado\n",
    "print(\"\\\\nüßÆ PREGUNTA 3:\")\n",
    "respuesta3 = ejecutor_matematico.invoke({\n",
    "    \"input\": \"Necesito calcular 25 * 4 y tambi√©n saber qu√© hora es\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta3['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc5902",
   "metadata": {},
   "source": [
    "## Agente 2: Consultor de Programaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo agente: Especialista en programaci√≥n\n",
    "herramientas_programacion = [\n",
    "    generar_password,\n",
    "    hash_texto,\n",
    "    validar_json,\n",
    "    convertir_binario,\n",
    "    info_lenguaje_programacion\n",
    "]\n",
    "\n",
    "# Prompt para el agente de programaci√≥n\n",
    "prompt_programacion = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un consultor experto en programaci√≥n y sistemas para estudiantes.\n",
    "    \n",
    "    ESPECIALIDADES:\n",
    "    - Informaci√≥n sobre lenguajes de programaci√≥n\n",
    "    - Generaci√≥n de contrase√±as seguras\n",
    "    - Validaci√≥n de JSON\n",
    "    - Conversiones entre sistemas num√©ricos\n",
    "    - Generaci√≥n de hashes\n",
    "    \n",
    "    ESTILO:\n",
    "    - S√© did√°ctico y explicativo\n",
    "    - Proporciona ejemplos cuando sea √∫til\n",
    "    - Usa las herramientas cuando sea necesario\n",
    "    - Si no necesitas herramientas, responde con tu conocimiento\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Crear agente y ejecutor\n",
    "agente_programacion = create_tool_calling_agent(\n",
    "    llm_gemini,\n",
    "    herramientas_programacion,\n",
    "    prompt_programacion\n",
    ")\n",
    "\n",
    "ejecutor_programacion = AgentExecutor(\n",
    "    agent=agente_programacion,\n",
    "    tools=herramientas_programacion,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"üíª Agente de Programaci√≥n creado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos el agente de programaci√≥n\n",
    "print(\"=== PRUEBAS DEL AGENTE DE PROGRAMACI√ìN ===\")\n",
    "\n",
    "# Pregunta 1: Informaci√≥n sobre lenguaje\n",
    "print(\"\\\\nüêç PREGUNTA 1:\")\n",
    "respuesta1 = ejecutor_programacion.invoke({\n",
    "    \"input\": \"Dame informaci√≥n sobre Python y genera una contrase√±a segura de 20 caracteres\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta1['output']}\")\n",
    "\n",
    "# Pregunta 2: Validaci√≥n y conversi√≥n\n",
    "print(\"\\\\nüî¢ PREGUNTA 2:\")\n",
    "respuesta2 = ejecutor_programacion.invoke({\n",
    "    \"input\": \"Valida este JSON: {'nombre': 'Ana'} y convierte el n√∫mero 255 a binario\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta2['output']}\")\n",
    "\n",
    "# Pregunta 3: Hash y consulta\n",
    "print(\"\\\\nüîê PREGUNTA 3:\")\n",
    "respuesta3 = ejecutor_programacion.invoke({\n",
    "    \"input\": \"Genera el hash SHA256 de la palabra 'password' y expl√≠came qu√© es Rust\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta3['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b51ae",
   "metadata": {},
   "source": [
    "\n",
    "## ¬øPor qu√© utilizar LangChain?\n",
    "\n",
    "Aunque podr√≠as interactuar directamente con un modelo de lenguaje usando el SDK que ofrece cada proveedor (por ejemplo la API de **OpenAI**, el servidor local de **Ollama** o el endpoint de **LM Studio**), **LangChain** proporciona una capa de abstracci√≥n y orquestaci√≥n muy √∫til cuando necesitas construir agentes m√°s sofisticados:\n",
    "\n",
    "- **Unifica interfaces**: te permite cambiar entre distintos LLMs (open source o propietarios) sin modificar el resto de tu c√≥digo, porque expone una API com√∫n para modelos de chat, embeddings y vector stores.\n",
    "- **Encadena tareas**: facilita construir *chains* donde la salida de una llamada se usa como entrada de otra, incluyendo flujos de preguntas y respuestas, an√°lisis de datos o ejecuci√≥n de herramientas externas.\n",
    "- **Gesti√≥n de memoria**: ofrece componentes para almacenar el historial de conversaciones y recuperarlo, algo esencial para agentes conversacionales.\n",
    "- **Integraci√≥n de herramientas**: permite exponer funciones personalizadas (c√°lculos, b√∫squedas, consultas API, etc.) como herramientas que el modelo puede invocar cuando es necesario.\n",
    "- **Recuperaci√≥n aumentada (RAG)**: se integra con motores de vector y sistemas de embeddings para buscar documentos relevantes y combinarlos con la generaci√≥n del LLM.\n",
    "\n",
    "En resumen, LangChain act√∫a como el pegamento que conecta los diferentes bloques (modelos, herramientas, bases de datos) y te permite centrarte en la l√≥gica de tu agente en lugar de los detalles de cada SDK.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befb4e9",
   "metadata": {},
   "source": [
    "## Agente 3: Super Asistente con Todas las Herramientas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente completo que combina todas las herramientas\n",
    "todas_las_herramientas = [\n",
    "    # Herramientas matem√°ticas\n",
    "    calculadora_basica,\n",
    "    calcular_area_circulo,\n",
    "    convertir_temperatura,\n",
    "    obtener_fecha_hora,\n",
    "    # Herramientas de programaci√≥n\n",
    "    generar_password,\n",
    "    hash_texto,\n",
    "    validar_json,\n",
    "    convertir_binario,\n",
    "    info_lenguaje_programacion\n",
    "]\n",
    "\n",
    "# Prompt para el super asistente\n",
    "prompt_super_asistente = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un SUPER ASISTENTE para estudiantes de sistemas, potenciado por Google Gemini.\n",
    "    \n",
    "    TUS SUPERPODERES:\n",
    "    üßÆ MATEM√ÅTICAS: C√°lculos, √°reas, conversiones de temperatura\n",
    "    üíª PROGRAMACI√ìN: Info de lenguajes, passwords, JSON, hashes, conversiones binarias\n",
    "    üïê UTILIDADES: Fecha/hora actual\n",
    "    \n",
    "    MISI√ìN:\n",
    "    Ayudar a estudiantes de sistemas con cualquier consulta t√©cnica o acad√©mica.\n",
    "    \n",
    "    INSTRUCCIONES:\n",
    "    - Analiza la consulta del usuario\n",
    "    - Decide qu√© herramientas necesitas (puedes usar varias)\n",
    "    - Proporciona respuestas completas y educativas\n",
    "    - Si no necesitas herramientas, usa tu conocimiento de Google Gemini\n",
    "    - Siempre s√© √∫til, did√°ctico y amigable\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Crear el super agente\n",
    "super_agente = create_tool_calling_agent(\n",
    "    llm_gemini,\n",
    "    todas_las_herramientas,\n",
    "    prompt_super_asistente\n",
    ")\n",
    "\n",
    "super_ejecutor = AgentExecutor(\n",
    "    agent=super_agente,\n",
    "    tools=todas_las_herramientas,\n",
    "    verbose=True,\n",
    "    max_iterations=5  # M√°s iteraciones para consultas complejas\n",
    ")\n",
    "\n",
    "print(\"üöÄ SUPER ASISTENTE creado con Google Gemini y todas las herramientas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3880fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostramos el poder del Super Asistente\n",
    "print(\"=== DEMOSTRACIONES DEL SUPER ASISTENTE ===\")\n",
    "\n",
    "# Consulta compleja que requiere m√∫ltiples herramientas\n",
    "print(\"\\\\nüéØ CONSULTA COMPLEJA:\")\n",
    "respuesta_compleja = super_ejecutor.invoke({\n",
    "    \"input\": \"\"\"Ay√∫dame con mi proyecto de sistemas:\n",
    "    1. Calcula el √°rea de un c√≠rculo con radio 10\n",
    "    2. Convierte 100¬∞C a Fahrenheit  \n",
    "    3. Genera una contrase√±a de 15 caracteres\n",
    "    4. Dame informaci√≥n sobre JavaScript\n",
    "    5. Dime qu√© hora es ahora\"\"\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta_compleja['output']}\")\n",
    "\n",
    "# Consulta de programaci√≥n avanzada\n",
    "print(\"\\\\nüíª CONSULTA DE PROGRAMACI√ìN:\")\n",
    "respuesta_prog = super_ejecutor.invoke({\n",
    "    \"input\": \"Valida este JSON: {'usuario': 'admin', 'activo': true} y luego genera el hash MD5 de 'mi_password_secreto'\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta_prog['output']}\")\n",
    "\n",
    "# Consulta educativa\n",
    "print(\"\\\\nüìö CONSULTA EDUCATIVA:\")\n",
    "respuesta_edu = super_ejecutor.invoke({\n",
    "    \"input\": \"Expl√≠came la diferencia entre Python y Java, y convierte el n√∫mero 1024 a binario\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta_edu['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3af0",
   "metadata": {},
   "source": [
    "### Instalaci√≥n y configuraci√≥n\n",
    "\n",
    "Para seguir este notebook necesitas instalar varias librer√≠as. Si ya tienes un entorno con `langchain` y `chromadb` puedes omitir esta celda. En una m√°quina local con acceso a internet se pueden instalar as√≠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c8229",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-community langchain-core chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a185c",
   "metadata": {},
   "source": [
    "### Importaci√≥n de m√≥dulos\n",
    "\n",
    "Importamos las clases y funciones necesarias para construir el agente. Esto incluye el modelo local (por ejemplo `ChatOllama`), el motor de memoria, las herramientas y funciones auxiliares de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool, tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861e6ec",
   "metadata": {},
   "source": [
    "## Cadenas en LangChain\n",
    "\n",
    "Una **cadena** combina uno o m√°s componentes (prompts, modelos, transformaciones) para construir un flujo de ejecuci√≥n. LangChain incluye utilidades como `LLMChain` para encapsular un prompt y un modelo. Aqu√≠ tienes un ejemplo de cadena simple que genera una respuesta a partir de un template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cadena simple\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definimos un prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Dime un dato curioso sobre {tema}.\")\n",
    "\n",
    "# Instanciamos el modelo local (suponiendo que est√© en marcha)\n",
    "llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Creamos la cadena\n",
    "cadena = LLMChain(llm=llm_local, prompt=prompt)\n",
    "\n",
    "# Para ejecutarla proporcionar√≠amos las variables del template:\n",
    "# resultado = cadena.invoke({\"tema\": \"Colombia\"})\n",
    "# print(resultado)\n",
    "# Nota: descomenta estas l√≠neas para ejecutar con un modelo local en marcha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b09f95",
   "metadata": {},
   "source": [
    "## Memoria en LangChain\n",
    "\n",
    "Los agentes conversacionales necesitan recordar lo que ya se ha dicho. LangChain ofrece varias implementaciones de memoria, como `ConversationBufferMemory`, que almacena el historial de mensajes en orden. Puedes combinarla con un LLMChain o un agente para que el modelo reciba contexto en cada llamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de ConversationBufferMemory con un LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Creamos la memoria\n",
    "memoria_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Definimos el prompt\n",
    "prompt_memoria = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente amistoso.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{chat_history}\"),\n",
    "])\n",
    "\n",
    "# Creamos la cadena con memoria\n",
    "cadena_memoria = LLMChain(llm=llm_local, prompt=prompt_memoria, memory=memoria_chain)\n",
    "\n",
    "# Para usarla, invoca la cadena varias veces; la memoria conservar√° el historial:\n",
    "# respuesta1 = cadena_memoria.invoke({\"input\": \"Hola\"})\n",
    "# respuesta2 = cadena_memoria.invoke({\"input\": \"¬øQu√© me dijiste antes?\"})\n",
    "# print(respuesta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8351375",
   "metadata": {},
   "source": [
    "## Plantillas de Prompt (Prompt Templates)\n",
    "\n",
    "LangChain facilita la construcci√≥n de prompts complejos mediante plantillas parametrizadas. Puedes combinar mensajes de sistema, de usuario y del asistente para definir el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba42c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de plantilla de prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "plantilla = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en matem√°ticas.\"),\n",
    "    (\"human\", \"Pregunta: {pregunta}\"),\n",
    "])\n",
    "\n",
    "# Instanciar√≠amos el modelo y llamar√≠amos:\n",
    "# llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "# chain = LLMChain(llm=llm_local, prompt=plantilla)\n",
    "# respuesta = chain.invoke({\"pregunta\": \"¬øCu√°nto es 12√ó8?\"})\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906b3f",
   "metadata": {},
   "source": [
    "## 1. Definir herramientas personalizadas\n",
    "\n",
    "Las **herramientas** permiten que el agente ejecute funciones espec√≠ficas (por ejemplo c√°lculos o b√∫squedas). Definiremos dos funciones sencillas que suman y multiplican n√∫meros. Utilizamos el decorador `@tool` para convertirlas en herramientas que LangChain pueda invocar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    # Suma dos n√∫meros y devuelve el resultado\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    # Multiplica dos n√∫meros y devuelve el resultado\n",
    "    return a * b\n",
    "\n",
    "# Registrar las herramientas en una lista\n",
    "herramientas = [sumar, multiplicar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696df71",
   "metadata": {},
   "source": [
    "## 2. Instanciar el modelo local\n",
    "\n",
    "Antes de crear el agente necesitamos un LLM local en ejecuci√≥n. Con **Ollama** podemos iniciar un servidor y cargar un modelo como *llama3:8b*:\n",
    "\n",
    "```bash\n",
    "# Instala Ollama (solo una vez)\n",
    "wget -qO- https://ollama.com/install.sh | sh\n",
    "# Arranca el servidor\n",
    "ollama serve &\n",
    "# Descarga y prepara el modelo (puede tardar unos minutos)\n",
    "ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "En **LM Studio** puedes descargar modelos desde la interfaz gr√°fica y exponer un endpoint local. Una vez en marcha, LangChain se conecta mediante la clase `ChatOllama` indicando el nombre del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2698f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el modelo local (aseg√∫rate de que el servidor de Ollama est√© ejecut√°ndose)\n",
    "# Si utilizas LM Studio, cambia el nombre del modelo o ajusta el endpoint.\n",
    "llm = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Tambi√©n puedes ajustar par√°metros como temperatura, top_p, etc.\n",
    "# llm = ChatOllama(model=\"llama3:8b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38857",
   "metadata": {},
   "source": [
    "## 3. Crear memoria y construir el agente\n",
    "\n",
    "La memoria mantiene el historial de conversaci√≥n. Usaremos `ConversationBufferMemory` para recordar los mensajes pasados. Luego construiremos un **agente reactivo** con `create_react_agent`, pasando el modelo, la lista de herramientas y la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una memoria de conversaci√≥n\n",
    "memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Crear el prompt que usar√° el agente\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente √∫til que puede usar herramientas para completar tareas.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente reactivo (ReAct)\n",
    "agente = create_react_agent(llm, herramientas, prompt)\n",
    "\n",
    "# Ejecutar el agente dentro de un executor para gestionar el estado y la memoria\n",
    "executor = AgentExecutor(agent=agente, tools=herramientas, memory=memoria, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb9f",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el agente\n",
    "\n",
    "Ya podemos hacer consultas al agente. El agente decidir√° si necesita llamar a alguna herramienta para calcular o buscar informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta: combinaci√≥n de c√°lculo\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¬øCu√°nto es 5*7 y 3*8?\"}]}\n",
    "# Para probar el agente descomenta las l√≠neas siguientes cuando tengas el modelo local ejecut√°ndose:\n",
    "respuesta = executor.invoke(pregunta)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62883c8f",
   "metadata": {},
   "source": [
    "## 5. Ejemplo de agente calculadora\n",
    "\n",
    "Construiremos un agente que s√≥lo utiliza herramientas aritm√©ticas para resolver preguntas de c√°lculo. El agente decidir√° si debe llamar a la funci√≥n de suma o multiplicaci√≥n en funci√≥n de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuevamente nuestras herramientas aritm√©ticas\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    return a * b\n",
    "\n",
    "# Lista de herramientas\n",
    "calculadora_tools = [sumar, multiplicar]\n",
    "\n",
    "# Instanciamos un modelo local (por ejemplo, llama3)\n",
    "llm_calc = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Memoria para el agente\n",
    "mem_calc = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt base\n",
    "prompt_calc = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres una calculadora que puede usar herramientas para sumar y multiplicar.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear agente y executor\n",
    "agente_calc = create_react_agent(llm_calc, calculadora_tools, prompt_calc)\n",
    "executor_calc = AgentExecutor(agent=agente_calc, tools=calculadora_tools, memory=mem_calc, verbose=True)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¬øCu√°nto es 4*6 m√°s 10?\"}]}\n",
    "# respuesta = executor_calc.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693985",
   "metadata": {},
   "source": [
    "## 6. Agente de b√∫squeda de art√≠culos cient√≠ficos con Playwright, Gemini y Ollama\n",
    "\n",
    "En este ejemplo final combinamos la automatizaci√≥n web mediante **Playwright** con dos modelos de lenguaje distintos. El agente navega a una base de datos de art√≠culos (por ejemplo Google Scholar), realiza una b√∫squeda de art√≠culos cient√≠ficos sobre un tema y extrae los t√≠tulos. Luego utiliza un modelo de **Gemini** para filtrar y evaluar la relevancia de los resultados, y finalmente emplea un modelo local de **Ollama** para sintetizar una respuesta para el usuario.\n",
    "\n",
    "**Nota:** Playwright necesita instalarse (`pip install playwright` y luego `playwright install`), y el acceso a Gemini requiere configurar credenciales de Google generative AI. Este ejemplo es ilustrativo y no se ejecutar√° en este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de agente que usa Playwright para buscar art√≠culos y combina dos modelos\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "@tool\n",
    "def buscar_articulos(topic: str) -> str:\n",
    "    \"\"\"Busca art√≠culos cient√≠ficos sobre un tema utilizando Playwright y devuelve los t√≠tulos encontrados.\"\"\"\n",
    "    from playwright.sync_api import sync_playwright\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        # Navegar a Google Scholar\n",
    "        page.goto(\"https://scholar.google.com\")\n",
    "        page.fill(\"input[name=q]\", topic)\n",
    "        page.press(\"input[name=q]\", \"Enter\")\n",
    "        page.wait_for_selector(\"h3\")\n",
    "        # Extraer los primeros 5 t√≠tulos\n",
    "        titles = page.eval_on_selector_all(\"h3\", \"elements => elements.slice(0,5).map(e => e.innerText)\")\n",
    "        browser.close()\n",
    "        return \"\".join(titles)\n",
    "\n",
    "# Instanciamos los modelos\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "llm_ollama = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Lista de herramientas\n",
    "tools_playwright = [buscar_articulos]\n",
    "\n",
    "# Prompt base para el agente\n",
    "prompt_playwright = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente investigador que puede usar herramientas para buscar y evaluar art√≠culos.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente que decide con Gemini\n",
    "agente_playwright = create_react_agent(llm_gemini, tools_playwright, prompt_playwright)\n",
    "\n",
    "# Executor que sintetiza la respuesta final usando Ollama\n",
    "mem_playwright = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "executor_playwright = AgentExecutor(agent=agente_playwright, tools=tools_playwright, memory=mem_playwright, verbose=True, llm=llm_ollama)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"Busca art√≠culos sobre inteligencia artificial explicable.\"}]}\n",
    "# respuesta = executor_playwright.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
