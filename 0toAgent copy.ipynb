{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac40623",
   "metadata": {},
   "source": [
    "# Workshop: De cero a Agente con LangChain y Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4245e",
   "metadata": {},
   "source": [
    "## Conexión directa a un LLM sin LangChain\n",
    "\n",
    "Antes de introducir LangChain, veamos cómo podríamos interactuar con un modelo de lenguaje utilizando únicamente el SDK o la API que proporciona el modelo. Por ejemplo, un servidor de Ollama expone un endpoint HTTP `http://localhost:11434/api/generate` donde puedes enviar un prompt y recibir la respuesta del modelo en formato JSON. De forma análoga, otros proveedores (como OpenAI) ofrecen SDKs o endpoints REST para invocar sus modelos.\n",
    "\n",
    "Interactuar de forma directa es útil para pruebas sencillas, pero pronto verás que gestionar memoria de conversación, combinar varios modelos, reintentar peticiones o integrar fuentes de datos externas se vuelve complejo. Aquí es donde entra LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46316392",
   "metadata": {},
   "source": [
    "# Ollama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c6699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "LangChain es un proyecto de inteligencia artificial basado en el modelo deep learning que se centra en la generación automática de textos. Según lo conocido, el modelo, llamado LLaMA (Large Language Model), es un redujo computacional que capaz(es) a las personas no programadas para lidiar con texto (en español) y a través de procesos de inteligencia artificial (IA) al intentar entender, Analizar e interpretar datos. \n",
      "\n",
      "El langchain se centra en la generación automática de texto, lo que significa que el modelo puede create textos nunca vistos o entendidos como si estuviera bien adentuw. Puedes usar el langchain para tasks como traducir texto a español o para creer texto personalized basado en datos.\n",
      "\n",
      "Sin embargo, es importante notar que el langchain no himself es una变形 ou veritable de la IA; su objetivo principal es solely focused en la generación automática de textos.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de llamada directa a un modelo local de Ollama\n",
    "import requests\n",
    "\n",
    "# Definimos el payload de la solicitud\n",
    "data = {\n",
    "    \"model\": \"deepseek-r1:1.5b\",\n",
    "    \"prompt\": \"Que es langchain?\",\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Realizamos la petición POST al endpoint de Ollama\n",
    "# (Nota: esta llamada sólo funcionará si tienes ollama corriendo de forma local)\n",
    "response = requests.post(\"http://localhost:11434/api/generate\", json=data)\n",
    "print(response.json()[\"response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0303e",
   "metadata": {},
   "source": [
    "# OpenRouterAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1d789a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (1.75.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/agents/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b5cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**LangChain es un *framework de código abierto* para desarrollar aplicaciones impulsadas por modelos de lenguaje (LLMs) como GPT, Claude, Llama, etc.** Su objetivo principal es **simplificar la creación de aplicaciones complejas que combinan LLMs con otras fuentes de datos o servicios, y gestionar sus interacciones de manera flexible y potente.**\n",
      "\n",
      "Piensa en LangChain como un conjunto de \"piezas de LEGO\" estandarizadas que te permiten:\n",
      "\n",
      "1.  **Conectar LLMs** fácilmente (OpenAI, Anthropic, Hugging Face, etc.).\n",
      "2.  **Integrar Datos Externos** (APIs, bases de datos, documentos PDF, hojas de cálculo, sitios web).\n",
      "3.  **Secuenciar Llamadas** (\"encadenar\" pasos donde la salida de un LLM se convierte en la entrada del siguiente o de una acción externa).\n",
      "4.  **Manejar el Estado** (recordar el historial de la conversación, contexto).\n",
      "5.  **Incorporar Memoria** (para aplicaciones conversacionales).\n",
      "\n",
      "### ¿Por qué lo necesitas?\n",
      "Usar directamente la API de un LLM para una pregunta sencilla está bien (`modelo.generate(\"¿Qué es el sol?\")`). Pero para aplicaciones reales necesitas:\n",
      "\n",
      "*   **Consultar documentos personales o bases de datos:** \"Resume este informe PDF para mi jefe\".\n",
      "*   **Tomar decisiones secuenciales:** \"Analiza estos datos, saca conclusiones y luego escribe un correo con ellas\".\n",
      "*   **Interactuar con herramientas:** \"Busca el clima actual en Buenos Aires y sugiere qué ropa llevar\".\n",
      "*   **Mantener conversaciones coherentes:** Recordar lo dicho antes en un chat.\n",
      "\n",
      "**LangChain proporciona los componentes (\"abstracciones\") para hacer esto de forma estructurada y reutilizable.**\n",
      "\n",
      "### Conceptos Clave (Componentes)\n",
      "\n",
      "1.  **Models (Modelos):** La capa de los LLMs (OpenAI GPT, Anthropic Claude, etc.) o modelos incrustados (Embeddings).\n",
      "2.  **Prompts (Indicaciones):** Plantillas para estructurar y optimizar la entrada al modelo (prompt engineering).\n",
      "3.  **Chains (Cadenas):** **El núcleo**. Combinaciones de llamadas a modelos, lógica y otros componentes en una secuencia. Pueden ser simples (Prompt -> LLM) o complejas.\n",
      "    *   **LCEL (LangChain Expression Language):** Sintaxis declarativa para construir cadenas complejas de forma fluida.\n",
      "4.  **Agents (Agentes):** Componentes que permiten al LLM **tomar decisiones** dinámicas sobre qué herramienta usar y en qué orden para resolver una tarea compleja. El \"cerebro\" que orquesta.\n",
      "5.  **Tools (Herramientas):** Funciones que el Agente puede llamar (buscar en Google, consultar una API, una calculadora, etc.).\n",
      "6.  **Memory (Memoria):** Mecanismo para persistir el estado/conversación entre interacciones con el modelo.\n",
      "7.  **Retrieval (Recuperación):** Para conectar grandes volúmenes de datos propios con el LLM, clave para RAG (\"Retrieval-Augmented Generation\"). Incluye embeddings y bases de datos vectoriales.\n",
      "8.  **Output Parsers (Parsers de Salida):** Convierten la salida desestructurada del LLM (texto) en un formato estructurado (JSON, lista, etc.).\n",
      "\n",
      "### Analogía Simple\n",
      "Imagina que construir una app con LLM es como construir una fábrica:\n",
      "\n",
      "*   **Los LLMs** son trabajadores inteligentes pero ingenuos.\n",
      "*   **Las Tools** son máquinas especializadas (impresoras, cortadoras).\n",
      "*   **Los Agents** son los capataces que deciden qué trabajador hace qué tarea y qué máquina usar según las órdenes.\n",
      "*   **La Memory** son los registros de producción pasada.\n",
      "*   **La Retrieval** es el almacén de materias primas propias.\n",
      "*   **Los Chains** son las líneas de montaje (pasos fijos).\n",
      "*   **LangChain** es el kit de construcción estandarizado y los planos para diseñar toda la fábrica.\n",
      "\n",
      "### Ejemplos de Aplicaciones (Lo que puedes construir)\n",
      "\n",
      "*   **Chatbots sofisticados:** Que consulten bases de conocimientos internas (RAG).\n",
      "*   **Agentes Analíticos:** Que analicen datos, generen reportes, alerten.\n",
      "*   **Asistentes de Código:** Que entiendan y modifiquen un repositorio entero.\n",
      "*   **Resumidores de Documentación:** Personalizados.\n",
      "*   **Interfaces de conversación con datos internos:** Pregunta a tu base de datos con lenguaje natural.\n",
      "*   **Automatización de flujos de trabajo:** Generación de contenido, clasificación, extracción de datos + toma de acciones posteriores.\n",
      "\n",
      "### Ejemplo mínimo de Código (Python)\n",
      "\n",
      "```python\n",
      "from langchain_openai import ChatOpenAI\n",
      "from langchain_core.prompts import ChatPromptTemplate\n",
      "\n",
      "# 1. Modelo\n",
      "llm = ChatOpenAI(api_key=\"tu_api_key\", model=\"gpt-3.5-turbo\")\n",
      "\n",
      "# 2. Prompt (Plantilla)\n",
      "prompt = ChatPromptTemplate.from_template(\n",
      "    \"Traduce al {lenguaje} el siguiente texto: {texto}\"\n",
      ")\n",
      "\n",
      "# 3. Chain SIMPLE (Prompt -> LLM)\n",
      "chain = prompt | llm  # LCEL: Usa el operador pipe '|'\n",
      "\n",
      "# Ejecutar\n",
      "respuesta = chain.invoke({\n",
      "    \"lenguaje\": \"francés\",\n",
      "    \"texto\": \"¡Hola! ¿Cómo estás?\"\n",
      "})\n",
      "print(respuesta.content)  # Salida: \"Bonjour ! Comment ça va ?\"\n",
      "```\n",
      "\n",
      "**En resumen:** LangChain es el *framework imprescindible* para pasar de hacer preguntas puntuales a LLMs a construir aplicaciones robustas, integradas y autónomas que aprovechen todo su potencial combinado con tus datos y sistemas. Es la capa de orquestación y abstracción sobre los LLMs.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=\"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\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "  extra_headers={\n",
    "    \"HTTP-Referer\": \"MI PAGINA o APP\", \n",
    "    \"X-Title\": \"ANDY CODE\",\n",
    "  },\n",
    "  model=\"deepseek/deepseek-r1-0528:free\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Que es langchain?\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14f5ab",
   "metadata": {},
   "source": [
    "# Google SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0134f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e164470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It learns patterns from data to make predictions or decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce15c64",
   "metadata": {},
   "source": [
    "# Tools (Herramientas) Personalizadas con Google Gemini\n",
    "\n",
    "Ahora vamos a aprender a crear herramientas personalizadas que los modelos pueden usar. Utilizaremos Google Gemini como nuestro modelo principal para todos los ejemplos.\n",
    "\n",
    "## ¿Qué son las Tools?\n",
    "\n",
    "🔧 **Funciones** que los LLMs pueden invocar para realizar tareas específicas  \n",
    "🌐 **Extensiones** de las capacidades básicas del modelo  \n",
    "⚡ **Automatización** de procesos complejos  \n",
    "🎯 **Integración** con sistemas externos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9863301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Google Gemini configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "# Configuración inicial con Google Gemini\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Configurar Google API\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDKPdCv74mFw9TsjWnqjWLTazlBSIncocs\"\n",
    "\n",
    "# Crear instancia del modelo Gemini\n",
    "llm_gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"✅ Google Gemini configurado correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4827",
   "metadata": {},
   "source": [
    "## Ejemplo 1: Herramientas Matemáticas Básicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d47c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PRUEBAS DE HERRAMIENTAS ===\n",
      "Resultado: 15 + 25 = 40\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for calcular_area_circulo\n  Input should be a valid dictionary or instance of calcular_area_circulo [type=model_type, input_value=5, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== PRUEBAS DE HERRAMIENTAS ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(calculadora_basica.invoke(\u001b[33m\"\u001b[39m\u001b[33m15 + 25\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcalcular_area_circulo\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(convertir_temperatura.invoke(\u001b[32m25\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(obtener_fecha_hora.invoke(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:513\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    505\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs: Any,\n\u001b[32m    511\u001b[39m ) -> Any:\n\u001b[32m    512\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:774\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    773\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    775\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    776\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:736\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    734\u001b[39m child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     tool_args, tool_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_to_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m signature(\u001b[38;5;28mself\u001b[39m._run).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    740\u001b[39m         tool_kwargs = tool_kwargs | {\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m: run_manager}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:651\u001b[39m, in \u001b[36mBaseTool._to_args_and_kwargs\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    644\u001b[39m     \u001b[38;5;28mself\u001b[39m.args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    645\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.args_schema, \u001b[38;5;28mtype\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m ):\n\u001b[32m    649\u001b[39m     \u001b[38;5;66;03m# StructuredTool with no args\u001b[39;00m\n\u001b[32m    650\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (), {}\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m tool_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_call_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[38;5;66;03m# For backwards compatibility, if run_input is a string,\u001b[39;00m\n\u001b[32m    653\u001b[39m \u001b[38;5;66;03m# pass as a positional argument.\u001b[39;00m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_input, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/langchain_core/tools/base.py:570\u001b[39m, in \u001b[36mBaseTool._parse_input\u001b[39m\u001b[34m(self, tool_input, tool_call_id)\u001b[39m\n\u001b[32m    568\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    569\u001b[39m             tool_input[k] = tool_call_id\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     result = \u001b[43minput_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m     result_dict = result.model_dump()\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(input_args, BaseModelV1):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agents/lib/python3.11/site-packages/pydantic/main.py:705\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    701\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    702\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for calcular_area_circulo\n  Input should be a valid dictionary or instance of calcular_area_circulo [type=model_type, input_value=5, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"
     ]
    }
   ],
   "source": [
    "# Creamos herramientas matemáticas para estudiantes\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def calculadora_basica(operacion: str) -> str:\n",
    "    \"\"\"Realiza operaciones matemáticas básicas. Formato: 'numero operador numero' (ej: '5 + 3', '10 * 2')\"\"\"\n",
    "    try:\n",
    "        # Operaciones permitidas para seguridad\n",
    "        operadores = {'+': '+', '-': '-', '*': '*', '/': '/', '^': '**'}\n",
    "        \n",
    "        # Reemplazar ^ por **\n",
    "        operacion = operacion.replace('^', '**')\n",
    "        \n",
    "        # Evaluar de forma segura\n",
    "        resultado = eval(operacion, {\"__builtins__\": {}}, {})\n",
    "        return f\"Resultado: {operacion} = {resultado}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error en el cálculo: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def calcular_area_circulo(radio: float) -> str:\n",
    "    \"\"\"Calcula el área de un círculo dado su radio\"\"\"\n",
    "    area = math.pi * (radio ** 2)\n",
    "    return f\"El área del círculo con radio {radio} es: {area:.2f} unidades cuadradas\"\n",
    "\n",
    "@tool\n",
    "def convertir_temperatura(valor: float, de_unidad: str, a_unidad: str) -> str:\n",
    "    \"\"\"Convierte temperaturas entre Celsius (C), Fahrenheit (F) y Kelvin (K)\"\"\"\n",
    "    de_unidad = de_unidad.upper()\n",
    "    a_unidad = a_unidad.upper()\n",
    "    \n",
    "    # Convertir todo a Celsius primero\n",
    "    if de_unidad == \"F\":\n",
    "        celsius = (valor - 32) * 5/9\n",
    "    elif de_unidad == \"K\":\n",
    "        celsius = valor - 273.15\n",
    "    else:\n",
    "        celsius = valor\n",
    "    \n",
    "    # Convertir de Celsius a la unidad destino\n",
    "    if a_unidad == \"F\":\n",
    "        resultado = celsius * 9/5 + 32\n",
    "    elif a_unidad == \"K\":\n",
    "        resultado = celsius + 273.15\n",
    "    else:\n",
    "        resultado = celsius\n",
    "    \n",
    "    return f\"{valor}° {de_unidad} = {resultado:.2f}° {a_unidad}\"\n",
    "\n",
    "@tool\n",
    "def obtener_fecha_hora() -> str:\n",
    "    \"\"\"Obtiene la fecha y hora actual\"\"\"\n",
    "    ahora = datetime.now()\n",
    "    return f\"Fecha y hora actual: {ahora.strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "# Probamos las herramientas individualmente\n",
    "print(\"=== PRUEBAS DE HERRAMIENTAS ===\")\n",
    "print(calculadora_basica.invoke(\"15 + 25\"))\n",
    "print(calcular_area_circulo.invoke(5))\n",
    "print(convertir_temperatura.invoke(25, \"C\", \"F\"))\n",
    "print(obtener_fecha_hora.invoke(\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992134e",
   "metadata": {},
   "source": [
    "## Ejemplo 2: Herramientas para Estudiantes de Sistemas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Herramientas específicas para programación y sistemas\n",
    "import json\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "@tool\n",
    "def generar_password(longitud: int = 12) -> str:\n",
    "    \"\"\"Genera una contraseña segura de la longitud especificada\"\"\"\n",
    "    import string\n",
    "    caracteres = string.ascii_letters + string.digits + \"!@#$%^&*\"\n",
    "    password = ''.join(random.choice(caracteres) for _ in range(longitud))\n",
    "    return f\"Contraseña generada: {password}\"\n",
    "\n",
    "@tool\n",
    "def hash_texto(texto: str, algoritmo: str = \"sha256\") -> str:\n",
    "    \"\"\"Genera el hash de un texto usando diferentes algoritmos (md5, sha1, sha256)\"\"\"\n",
    "    try:\n",
    "        if algoritmo == \"md5\":\n",
    "            hash_obj = hashlib.md5(texto.encode())\n",
    "        elif algoritmo == \"sha1\":\n",
    "            hash_obj = hashlib.sha1(texto.encode())\n",
    "        elif algoritmo == \"sha256\":\n",
    "            hash_obj = hashlib.sha256(texto.encode())\n",
    "        else:\n",
    "            return \"Algoritmo no soportado. Use: md5, sha1, sha256\"\n",
    "        \n",
    "        return f\"Hash {algoritmo} de '{texto}': {hash_obj.hexdigest()}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generando hash: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def validar_json(json_string: str) -> str:\n",
    "    \"\"\"Valida si una cadena es un JSON válido\"\"\"\n",
    "    try:\n",
    "        json.loads(json_string)\n",
    "        return \"✅ El JSON es válido\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        return f\"❌ JSON inválido: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def convertir_binario(numero: int, conversion: str) -> str:\n",
    "    \"\"\"Convierte números entre decimal, binario, octal y hexadecimal\"\"\"\n",
    "    try:\n",
    "        if conversion == \"decimal_a_binario\":\n",
    "            return f\"{numero} en binario: {bin(numero)}\"\n",
    "        elif conversion == \"decimal_a_octal\":\n",
    "            return f\"{numero} en octal: {oct(numero)}\"\n",
    "        elif conversion == \"decimal_a_hex\":\n",
    "            return f\"{numero} en hexadecimal: {hex(numero)}\"\n",
    "        elif conversion == \"binario_a_decimal\":\n",
    "            decimal = int(str(numero), 2)\n",
    "            return f\"{numero} (binario) en decimal: {decimal}\"\n",
    "        else:\n",
    "            return \"Conversiones disponibles: decimal_a_binario, decimal_a_octal, decimal_a_hex, binario_a_decimal\"\n",
    "    except Exception as e:\n",
    "        return f\"Error en conversión: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def info_lenguaje_programacion(lenguaje: str) -> str:\n",
    "    \"\"\"Proporciona información básica sobre lenguajes de programación\"\"\"\n",
    "    lenguajes = {\n",
    "        \"python\": \"🐍 Python: Lenguaje interpretado, fácil de aprender, usado en IA, web, automatización\",\n",
    "        \"javascript\": \"⚡ JavaScript: Lenguaje para desarrollo web, tanto frontend como backend (Node.js)\",\n",
    "        \"java\": \"☕ Java: Lenguaje compilado, orientado a objetos, usado en empresas y Android\",\n",
    "        \"c++\": \"⚙️ C++: Lenguaje de bajo nivel, rápido, usado en sistemas y videojuegos\",\n",
    "        \"go\": \"🚀 Go: Lenguaje de Google, rápido, usado en microservicios y sistemas distribuidos\",\n",
    "        \"rust\": \"🦀 Rust: Lenguaje seguro en memoria, rápido, usado en sistemas y blockchain\"\n",
    "    }\n",
    "    \n",
    "    lenguaje_lower = lenguaje.lower()\n",
    "    if lenguaje_lower in lenguajes:\n",
    "        return lenguajes[lenguaje_lower]\n",
    "    else:\n",
    "        return f\"No tengo información sobre '{lenguaje}'. Lenguajes disponibles: {', '.join(lenguajes.keys())}\"\n",
    "\n",
    "# Probamos las herramientas de sistemas\n",
    "print(\"=== HERRAMIENTAS PARA SISTEMAS ===\")\n",
    "print(generar_password.invoke(16))\n",
    "print(hash_texto.invoke(\"Hello World\", \"sha256\"))\n",
    "print(validar_json.invoke('{\"nombre\": \"Juan\", \"edad\": 25}'))\n",
    "print(convertir_binario.invoke(42, \"decimal_a_binario\"))\n",
    "print(info_lenguaje_programacion.invoke(\"python\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765e8476",
   "metadata": {},
   "source": [
    "# Agentes Sencillos con Google Gemini\n",
    "\n",
    "Ahora vamos a crear agentes que pueden tomar decisiones y usar herramientas de forma autónoma. Un agente combina un LLM con herramientas y puede decidir qué acciones tomar.\n",
    "\n",
    "## ¿Qué es un Agente?\n",
    "\n",
    "🤖 **Sistema autónomo** que toma decisiones  \n",
    "🔧 **Usa herramientas** según sea necesario  \n",
    "🧠 **Razona** sobre qué hacer  \n",
    "🔄 **Itera** hasta resolver el problema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos las dependencias necesarias para agentes\n",
    "%pip install langchain-core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b29dde",
   "metadata": {},
   "source": [
    "## Agente 1: Asistente Matemático\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuestro primer agente simple\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Definir las herramientas matemáticas para el agente\n",
    "herramientas_matematicas = [\n",
    "    calculadora_basica,\n",
    "    calcular_area_circulo,\n",
    "    convertir_temperatura,\n",
    "    obtener_fecha_hora\n",
    "]\n",
    "\n",
    "# Crear el prompt para el agente matemático\n",
    "prompt_matematico = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente matemático especializado para estudiantes.\n",
    "    \n",
    "    CAPACIDADES:\n",
    "    - Realizar cálculos básicos (+, -, *, /)\n",
    "    - Calcular áreas de círculos\n",
    "    - Convertir temperaturas\n",
    "    - Obtener fecha y hora actual\n",
    "    \n",
    "    INSTRUCCIONES:\n",
    "    - Usa las herramientas disponibles cuando sea necesario\n",
    "    - Explica paso a paso los cálculos\n",
    "    - Si no necesitas herramientas, responde directamente\n",
    "    - Sé didáctico y claro en tus explicaciones\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Crear el agente\n",
    "agente_matematico = create_tool_calling_agent(\n",
    "    llm_gemini, \n",
    "    herramientas_matematicas, \n",
    "    prompt_matematico\n",
    ")\n",
    "\n",
    "# Crear el ejecutor del agente\n",
    "ejecutor_matematico = AgentExecutor(\n",
    "    agent=agente_matematico,\n",
    "    tools=herramientas_matematicas,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"🧮 Agente Matemático creado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d9237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos el agente matemático\n",
    "print(\"=== PRUEBAS DEL AGENTE MATEMÁTICO ===\")\n",
    "\n",
    "# Pregunta 1: Cálculo simple\n",
    "print(\"\\\\n🔢 PREGUNTA 1:\")\n",
    "respuesta1 = ejecutor_matematico.invoke({\n",
    "    \"input\": \"Calcula el área de un círculo con radio 7 metros\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta1['output']}\")\n",
    "\n",
    "# Pregunta 2: Conversión de temperatura  \n",
    "print(\"\\\\n🌡️ PREGUNTA 2:\")\n",
    "respuesta2 = ejecutor_matematico.invoke({\n",
    "    \"input\": \"Si afuera hace 85 grados Fahrenheit, ¿cuántos grados Celsius son?\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta2['output']}\")\n",
    "\n",
    "# Pregunta 3: Problema combinado\n",
    "print(\"\\\\n🧮 PREGUNTA 3:\")\n",
    "respuesta3 = ejecutor_matematico.invoke({\n",
    "    \"input\": \"Necesito calcular 25 * 4 y también saber qué hora es\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta3['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bc5902",
   "metadata": {},
   "source": [
    "## Agente 2: Consultor de Programación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo agente: Especialista en programación\n",
    "herramientas_programacion = [\n",
    "    generar_password,\n",
    "    hash_texto,\n",
    "    validar_json,\n",
    "    convertir_binario,\n",
    "    info_lenguaje_programacion\n",
    "]\n",
    "\n",
    "# Prompt para el agente de programación\n",
    "prompt_programacion = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un consultor experto en programación y sistemas para estudiantes.\n",
    "    \n",
    "    ESPECIALIDADES:\n",
    "    - Información sobre lenguajes de programación\n",
    "    - Generación de contraseñas seguras\n",
    "    - Validación de JSON\n",
    "    - Conversiones entre sistemas numéricos\n",
    "    - Generación de hashes\n",
    "    \n",
    "    ESTILO:\n",
    "    - Sé didáctico y explicativo\n",
    "    - Proporciona ejemplos cuando sea útil\n",
    "    - Usa las herramientas cuando sea necesario\n",
    "    - Si no necesitas herramientas, responde con tu conocimiento\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Crear agente y ejecutor\n",
    "agente_programacion = create_tool_calling_agent(\n",
    "    llm_gemini,\n",
    "    herramientas_programacion,\n",
    "    prompt_programacion\n",
    ")\n",
    "\n",
    "ejecutor_programacion = AgentExecutor(\n",
    "    agent=agente_programacion,\n",
    "    tools=herramientas_programacion,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "print(\"💻 Agente de Programación creado exitosamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos el agente de programación\n",
    "print(\"=== PRUEBAS DEL AGENTE DE PROGRAMACIÓN ===\")\n",
    "\n",
    "# Pregunta 1: Información sobre lenguaje\n",
    "print(\"\\\\n🐍 PREGUNTA 1:\")\n",
    "respuesta1 = ejecutor_programacion.invoke({\n",
    "    \"input\": \"Dame información sobre Python y genera una contraseña segura de 20 caracteres\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta1['output']}\")\n",
    "\n",
    "# Pregunta 2: Validación y conversión\n",
    "print(\"\\\\n🔢 PREGUNTA 2:\")\n",
    "respuesta2 = ejecutor_programacion.invoke({\n",
    "    \"input\": \"Valida este JSON: {'nombre': 'Ana'} y convierte el número 255 a binario\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta2['output']}\")\n",
    "\n",
    "# Pregunta 3: Hash y consulta\n",
    "print(\"\\\\n🔐 PREGUNTA 3:\")\n",
    "respuesta3 = ejecutor_programacion.invoke({\n",
    "    \"input\": \"Genera el hash SHA256 de la palabra 'password' y explícame qué es Rust\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta3['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b51ae",
   "metadata": {},
   "source": [
    "\n",
    "## ¿Por qué utilizar LangChain?\n",
    "\n",
    "Aunque podrías interactuar directamente con un modelo de lenguaje usando el SDK que ofrece cada proveedor (por ejemplo la API de **OpenAI**, el servidor local de **Ollama** o el endpoint de **LM Studio**), **LangChain** proporciona una capa de abstracción y orquestación muy útil cuando necesitas construir agentes más sofisticados:\n",
    "\n",
    "- **Unifica interfaces**: te permite cambiar entre distintos LLMs (open source o propietarios) sin modificar el resto de tu código, porque expone una API común para modelos de chat, embeddings y vector stores.\n",
    "- **Encadena tareas**: facilita construir *chains* donde la salida de una llamada se usa como entrada de otra, incluyendo flujos de preguntas y respuestas, análisis de datos o ejecución de herramientas externas.\n",
    "- **Gestión de memoria**: ofrece componentes para almacenar el historial de conversaciones y recuperarlo, algo esencial para agentes conversacionales.\n",
    "- **Integración de herramientas**: permite exponer funciones personalizadas (cálculos, búsquedas, consultas API, etc.) como herramientas que el modelo puede invocar cuando es necesario.\n",
    "- **Recuperación aumentada (RAG)**: se integra con motores de vector y sistemas de embeddings para buscar documentos relevantes y combinarlos con la generación del LLM.\n",
    "\n",
    "En resumen, LangChain actúa como el pegamento que conecta los diferentes bloques (modelos, herramientas, bases de datos) y te permite centrarte en la lógica de tu agente en lugar de los detalles de cada SDK.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5befb4e9",
   "metadata": {},
   "source": [
    "## Agente 3: Super Asistente con Todas las Herramientas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agente completo que combina todas las herramientas\n",
    "todas_las_herramientas = [\n",
    "    # Herramientas matemáticas\n",
    "    calculadora_basica,\n",
    "    calcular_area_circulo,\n",
    "    convertir_temperatura,\n",
    "    obtener_fecha_hora,\n",
    "    # Herramientas de programación\n",
    "    generar_password,\n",
    "    hash_texto,\n",
    "    validar_json,\n",
    "    convertir_binario,\n",
    "    info_lenguaje_programacion\n",
    "]\n",
    "\n",
    "# Prompt para el super asistente\n",
    "prompt_super_asistente = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un SUPER ASISTENTE para estudiantes de sistemas, potenciado por Google Gemini.\n",
    "    \n",
    "    TUS SUPERPODERES:\n",
    "    🧮 MATEMÁTICAS: Cálculos, áreas, conversiones de temperatura\n",
    "    💻 PROGRAMACIÓN: Info de lenguajes, passwords, JSON, hashes, conversiones binarias\n",
    "    🕐 UTILIDADES: Fecha/hora actual\n",
    "    \n",
    "    MISIÓN:\n",
    "    Ayudar a estudiantes de sistemas con cualquier consulta técnica o académica.\n",
    "    \n",
    "    INSTRUCCIONES:\n",
    "    - Analiza la consulta del usuario\n",
    "    - Decide qué herramientas necesitas (puedes usar varias)\n",
    "    - Proporciona respuestas completas y educativas\n",
    "    - Si no necesitas herramientas, usa tu conocimiento de Google Gemini\n",
    "    - Siempre sé útil, didáctico y amigable\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\")\n",
    "])\n",
    "\n",
    "# Crear el super agente\n",
    "super_agente = create_tool_calling_agent(\n",
    "    llm_gemini,\n",
    "    todas_las_herramientas,\n",
    "    prompt_super_asistente\n",
    ")\n",
    "\n",
    "super_ejecutor = AgentExecutor(\n",
    "    agent=super_agente,\n",
    "    tools=todas_las_herramientas,\n",
    "    verbose=True,\n",
    "    max_iterations=5  # Más iteraciones para consultas complejas\n",
    ")\n",
    "\n",
    "print(\"🚀 SUPER ASISTENTE creado con Google Gemini y todas las herramientas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3880fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostramos el poder del Super Asistente\n",
    "print(\"=== DEMOSTRACIONES DEL SUPER ASISTENTE ===\")\n",
    "\n",
    "# Consulta compleja que requiere múltiples herramientas\n",
    "print(\"\\\\n🎯 CONSULTA COMPLEJA:\")\n",
    "respuesta_compleja = super_ejecutor.invoke({\n",
    "    \"input\": \"\"\"Ayúdame con mi proyecto de sistemas:\n",
    "    1. Calcula el área de un círculo con radio 10\n",
    "    2. Convierte 100°C a Fahrenheit  \n",
    "    3. Genera una contraseña de 15 caracteres\n",
    "    4. Dame información sobre JavaScript\n",
    "    5. Dime qué hora es ahora\"\"\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta_compleja['output']}\")\n",
    "\n",
    "# Consulta de programación avanzada\n",
    "print(\"\\\\n💻 CONSULTA DE PROGRAMACIÓN:\")\n",
    "respuesta_prog = super_ejecutor.invoke({\n",
    "    \"input\": \"Valida este JSON: {'usuario': 'admin', 'activo': true} y luego genera el hash MD5 de 'mi_password_secreto'\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta_prog['output']}\")\n",
    "\n",
    "# Consulta educativa\n",
    "print(\"\\\\n📚 CONSULTA EDUCATIVA:\")\n",
    "respuesta_edu = super_ejecutor.invoke({\n",
    "    \"input\": \"Explícame la diferencia entre Python y Java, y convierte el número 1024 a binario\"\n",
    "})\n",
    "print(f\"Respuesta: {respuesta_edu['output']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3af0",
   "metadata": {},
   "source": [
    "### Instalación y configuración\n",
    "\n",
    "Para seguir este notebook necesitas instalar varias librerías. Si ya tienes un entorno con `langchain` y `chromadb` puedes omitir esta celda. En una máquina local con acceso a internet se pueden instalar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4c8229",
   "metadata": {
    "tags": [
     "bash"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain langchain-community langchain-core chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a185c",
   "metadata": {},
   "source": [
    "### Importación de módulos\n",
    "\n",
    "Importamos las clases y funciones necesarias para construir el agente. Esto incluye el modelo local (por ejemplo `ChatOllama`), el motor de memoria, las herramientas y funciones auxiliares de LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64059cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import Tool, tool\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861e6ec",
   "metadata": {},
   "source": [
    "## Cadenas en LangChain\n",
    "\n",
    "Una **cadena** combina uno o más componentes (prompts, modelos, transformaciones) para construir un flujo de ejecución. LangChain incluye utilidades como `LLMChain` para encapsular un prompt y un modelo. Aquí tienes un ejemplo de cadena simple que genera una respuesta a partir de un template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cadena simple\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Definimos un prompt\n",
    "prompt = ChatPromptTemplate.from_template(\"Dime un dato curioso sobre {tema}.\")\n",
    "\n",
    "# Instanciamos el modelo local (suponiendo que esté en marcha)\n",
    "llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Creamos la cadena\n",
    "cadena = LLMChain(llm=llm_local, prompt=prompt)\n",
    "\n",
    "# Para ejecutarla proporcionaríamos las variables del template:\n",
    "# resultado = cadena.invoke({\"tema\": \"Colombia\"})\n",
    "# print(resultado)\n",
    "# Nota: descomenta estas líneas para ejecutar con un modelo local en marcha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b09f95",
   "metadata": {},
   "source": [
    "## Memoria en LangChain\n",
    "\n",
    "Los agentes conversacionales necesitan recordar lo que ya se ha dicho. LangChain ofrece varias implementaciones de memoria, como `ConversationBufferMemory`, que almacena el historial de mensajes en orden. Puedes combinarla con un LLMChain o un agente para que el modelo reciba contexto en cada llamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de ConversationBufferMemory con un LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Creamos la memoria\n",
    "memoria_chain = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Definimos el prompt\n",
    "prompt_memoria = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente amistoso.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{chat_history}\"),\n",
    "])\n",
    "\n",
    "# Creamos la cadena con memoria\n",
    "cadena_memoria = LLMChain(llm=llm_local, prompt=prompt_memoria, memory=memoria_chain)\n",
    "\n",
    "# Para usarla, invoca la cadena varias veces; la memoria conservará el historial:\n",
    "# respuesta1 = cadena_memoria.invoke({\"input\": \"Hola\"})\n",
    "# respuesta2 = cadena_memoria.invoke({\"input\": \"¿Qué me dijiste antes?\"})\n",
    "# print(respuesta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8351375",
   "metadata": {},
   "source": [
    "## Plantillas de Prompt (Prompt Templates)\n",
    "\n",
    "LangChain facilita la construcción de prompts complejos mediante plantillas parametrizadas. Puedes combinar mensajes de sistema, de usuario y del asistente para definir el comportamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba42c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de plantilla de prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "plantilla = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en matemáticas.\"),\n",
    "    (\"human\", \"Pregunta: {pregunta}\"),\n",
    "])\n",
    "\n",
    "# Instanciaríamos el modelo y llamaríamos:\n",
    "# llm_local = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "# chain = LLMChain(llm=llm_local, prompt=plantilla)\n",
    "# respuesta = chain.invoke({\"pregunta\": \"¿Cuánto es 12×8?\"})\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94906b3f",
   "metadata": {},
   "source": [
    "## 1. Definir herramientas personalizadas\n",
    "\n",
    "Las **herramientas** permiten que el agente ejecute funciones específicas (por ejemplo cálculos o búsquedas). Definiremos dos funciones sencillas que suman y multiplican números. Utilizamos el decorador `@tool` para convertirlas en herramientas que LangChain pueda invocar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb4ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    # Suma dos números y devuelve el resultado\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    # Multiplica dos números y devuelve el resultado\n",
    "    return a * b\n",
    "\n",
    "# Registrar las herramientas en una lista\n",
    "herramientas = [sumar, multiplicar]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696df71",
   "metadata": {},
   "source": [
    "## 2. Instanciar el modelo local\n",
    "\n",
    "Antes de crear el agente necesitamos un LLM local en ejecución. Con **Ollama** podemos iniciar un servidor y cargar un modelo como *llama3:8b*:\n",
    "\n",
    "```bash\n",
    "# Instala Ollama (solo una vez)\n",
    "wget -qO- https://ollama.com/install.sh | sh\n",
    "# Arranca el servidor\n",
    "ollama serve &\n",
    "# Descarga y prepara el modelo (puede tardar unos minutos)\n",
    "ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "En **LM Studio** puedes descargar modelos desde la interfaz gráfica y exponer un endpoint local. Una vez en marcha, LangChain se conecta mediante la clase `ChatOllama` indicando el nombre del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2698f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos el modelo local (asegúrate de que el servidor de Ollama esté ejecutándose)\n",
    "# Si utilizas LM Studio, cambia el nombre del modelo o ajusta el endpoint.\n",
    "llm = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# También puedes ajustar parámetros como temperatura, top_p, etc.\n",
    "# llm = ChatOllama(model=\"llama3:8b\", temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b38857",
   "metadata": {},
   "source": [
    "## 3. Crear memoria y construir el agente\n",
    "\n",
    "La memoria mantiene el historial de conversación. Usaremos `ConversationBufferMemory` para recordar los mensajes pasados. Luego construiremos un **agente reactivo** con `create_react_agent`, pasando el modelo, la lista de herramientas y la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9257bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una memoria de conversación\n",
    "memoria = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Crear el prompt que usará el agente\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil que puede usar herramientas para completar tareas.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente reactivo (ReAct)\n",
    "agente = create_react_agent(llm, herramientas, prompt)\n",
    "\n",
    "# Ejecutar el agente dentro de un executor para gestionar el estado y la memoria\n",
    "executor = AgentExecutor(agent=agente, tools=herramientas, memory=memoria, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9eb9f",
   "metadata": {},
   "source": [
    "## 4. Ejecutar el agente\n",
    "\n",
    "Ya podemos hacer consultas al agente. El agente decidirá si necesita llamar a alguna herramienta para calcular o buscar información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de consulta: combinación de cálculo\n",
    "pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuánto es 5*7 y 3*8?\"}]}\n",
    "# Para probar el agente descomenta las líneas siguientes cuando tengas el modelo local ejecutándose:\n",
    "respuesta = executor.invoke(pregunta)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62883c8f",
   "metadata": {},
   "source": [
    "## 5. Ejemplo de agente calculadora\n",
    "\n",
    "Construiremos un agente que sólo utiliza herramientas aritméticas para resolver preguntas de cálculo. El agente decidirá si debe llamar a la función de suma o multiplicación en función de la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos nuevamente nuestras herramientas aritméticas\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "\n",
    "@tool\n",
    "def sumar(a: float, b: float) -> float:\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiplicar(a: float, b: float) -> float:\n",
    "    return a * b\n",
    "\n",
    "# Lista de herramientas\n",
    "calculadora_tools = [sumar, multiplicar]\n",
    "\n",
    "# Instanciamos un modelo local (por ejemplo, llama3)\n",
    "llm_calc = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Memoria para el agente\n",
    "mem_calc = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Prompt base\n",
    "prompt_calc = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres una calculadora que puede usar herramientas para sumar y multiplicar.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear agente y executor\n",
    "agente_calc = create_react_agent(llm_calc, calculadora_tools, prompt_calc)\n",
    "executor_calc = AgentExecutor(agent=agente_calc, tools=calculadora_tools, memory=mem_calc, verbose=True)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"¿Cuánto es 4*6 más 10?\"}]}\n",
    "# respuesta = executor_calc.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693985",
   "metadata": {},
   "source": [
    "## 6. Agente de búsqueda de artículos científicos con Playwright, Gemini y Ollama\n",
    "\n",
    "En este ejemplo final combinamos la automatización web mediante **Playwright** con dos modelos de lenguaje distintos. El agente navega a una base de datos de artículos (por ejemplo Google Scholar), realiza una búsqueda de artículos científicos sobre un tema y extrae los títulos. Luego utiliza un modelo de **Gemini** para filtrar y evaluar la relevancia de los resultados, y finalmente emplea un modelo local de **Ollama** para sintetizar una respuesta para el usuario.\n",
    "\n",
    "**Nota:** Playwright necesita instalarse (`pip install playwright` y luego `playwright install`), y el acceso a Gemini requiere configurar credenciales de Google generative AI. Este ejemplo es ilustrativo y no se ejecutará en este entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de agente que usa Playwright para buscar artículos y combina dos modelos\n",
    "from langchain_core.tools import tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "@tool\n",
    "def buscar_articulos(topic: str) -> str:\n",
    "    \"\"\"Busca artículos científicos sobre un tema utilizando Playwright y devuelve los títulos encontrados.\"\"\"\n",
    "    from playwright.sync_api import sync_playwright\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        # Navegar a Google Scholar\n",
    "        page.goto(\"https://scholar.google.com\")\n",
    "        page.fill(\"input[name=q]\", topic)\n",
    "        page.press(\"input[name=q]\", \"Enter\")\n",
    "        page.wait_for_selector(\"h3\")\n",
    "        # Extraer los primeros 5 títulos\n",
    "        titles = page.eval_on_selector_all(\"h3\", \"elements => elements.slice(0,5).map(e => e.innerText)\")\n",
    "        browser.close()\n",
    "        return \"\".join(titles)\n",
    "\n",
    "# Instanciamos los modelos\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "llm_ollama = ChatOllama(model=\"llama3:8b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "# Lista de herramientas\n",
    "tools_playwright = [buscar_articulos]\n",
    "\n",
    "# Prompt base para el agente\n",
    "prompt_playwright = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente investigador que puede usar herramientas para buscar y evaluar artículos.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"agent\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# Crear el agente que decide con Gemini\n",
    "agente_playwright = create_react_agent(llm_gemini, tools_playwright, prompt_playwright)\n",
    "\n",
    "# Executor que sintetiza la respuesta final usando Ollama\n",
    "mem_playwright = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "executor_playwright = AgentExecutor(agent=agente_playwright, tools=tools_playwright, memory=mem_playwright, verbose=True, llm=llm_ollama)\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# pregunta = {\"messages\": [{\"role\": \"user\", \"content\": \"Busca artículos sobre inteligencia artificial explicable.\"}]}\n",
    "# respuesta = executor_playwright.invoke(pregunta)\n",
    "# print(respuesta)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
